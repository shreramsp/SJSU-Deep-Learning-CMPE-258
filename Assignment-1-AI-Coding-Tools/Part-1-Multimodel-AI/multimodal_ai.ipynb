{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal AI Demo - CMPE 258 Assignment\n",
    "\n",
    "This notebook demonstrates various multimodal AI capabilities using **free, open-source models**:\n",
    "1. **Text-to-Image Generation** using Stable Diffusion\n",
    "2. **Image Analysis** using BLIP (Salesforce)\n",
    "3. **Text Conversations** using Mistral (via Hugging Face)\n",
    "\n",
    "**No API keys required!** All models run locally in Colab.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "First, let's install all required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q diffusers transformers accelerate torch pillow sentencepiece protobuf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Text-to-Image Generation with Stable Diffusion\n",
    "\n",
    "We'll use Stable Diffusion v1.5 to generate images from text prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check if GPU is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Load Stable Diffusion model\n",
    "print(\"\\nLoading Stable Diffusion v1.5...\")\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
    ")\n",
    "pipe = pipe.to(device)\n",
    "\n",
    "print(\"‚úÖ Stable Diffusion model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate images with different prompts\n",
    "prompts = [\n",
    "    \"A futuristic city with flying cars at sunset, photorealistic, 8k\",\n",
    "    \"A magical forest with glowing mushrooms and fireflies, fantasy art\",\n",
    "    \"A robot playing chess with a human in a cozy library, oil painting style\"\n",
    "]\n",
    "\n",
    "generated_images = []\n",
    "\n",
    "print(\"Starting image generation...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, prompt in enumerate(prompts):\n",
    "    print(f\"\\n[Image {i+1}/{len(prompts)}]\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(\"Generating...\")\n",
    "    \n",
    "    # Generate image\n",
    "    image = pipe(prompt, num_inference_steps=30, guidance_scale=7.5).images[0]\n",
    "    generated_images.append(image)\n",
    "    \n",
    "    # Display the image\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.title(prompt, fontsize=12, wrap=True, pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save the image\n",
    "    filename = f\"generated_image_{i+1}.png\"\n",
    "    image.save(filename)\n",
    "    print(f\"‚úÖ Saved as {filename}\")\n",
    "    print(\"-\"*80)\n",
    "\n",
    "print(\"\\n‚úÖ All images generated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Image Analysis with BLIP\n",
    "\n",
    "We'll use Salesforce's BLIP (Bootstrapping Language-Image Pre-training) model to analyze the generated images.\n",
    "\n",
    "**BLIP is completely free and runs locally - no API key needed!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration, BlipForQuestionAnswering\n",
    "\n",
    "# Load BLIP models for image captioning and VQA\n",
    "print(\"Loading BLIP models...\\n\")\n",
    "\n",
    "# Model 1: Image Captioning\n",
    "print(\"Loading BLIP Image Captioning model...\")\n",
    "caption_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "caption_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\").to(device)\n",
    "print(\"‚úÖ Captioning model loaded\")\n",
    "\n",
    "# Model 2: Visual Question Answering\n",
    "print(\"Loading BLIP VQA model...\")\n",
    "vqa_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "vqa_model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\").to(device)\n",
    "print(\"‚úÖ VQA model loaded\")\n",
    "\n",
    "print(\"\\n‚úÖ All BLIP models ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze each generated image\n",
    "print(\"Starting image analysis...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, (image, prompt) in enumerate(zip(generated_images, prompts)):\n",
    "    print(f\"\\n[Analyzing Image {i+1}]\")\n",
    "    print(f\"Original Prompt: {prompt}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    # Display the image\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Image {i+1}\", fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 1. Generate unconditional caption\n",
    "    print(\"\\n1Ô∏è‚É£  General Description:\")\n",
    "    inputs = caption_processor(image, return_tensors=\"pt\").to(device)\n",
    "    out = caption_model.generate(**inputs, max_length=50)\n",
    "    caption = caption_processor.decode(out[0], skip_special_tokens=True)\n",
    "    print(f\"   {caption}\")\n",
    "    \n",
    "    # 2. Generate conditional caption (more detailed)\n",
    "    print(\"\\n2Ô∏è‚É£  Detailed Analysis:\")\n",
    "    text_prompt = \"a detailed description of\"\n",
    "    inputs = caption_processor(image, text_prompt, return_tensors=\"pt\").to(device)\n",
    "    out = caption_model.generate(**inputs, max_length=100)\n",
    "    detailed_caption = caption_processor.decode(out[0], skip_special_tokens=True)\n",
    "    print(f\"   {detailed_caption}\")\n",
    "    \n",
    "    # 3. Visual Question Answering\n",
    "    print(\"\\n3Ô∏è‚É£  Visual Q&A:\")\n",
    "    questions = [\n",
    "        \"What is the main subject of this image?\",\n",
    "        \"What is the mood or atmosphere?\",\n",
    "        \"What colors are dominant?\"\n",
    "    ]\n",
    "    \n",
    "    for question in questions:\n",
    "        inputs = vqa_processor(image, question, return_tensors=\"pt\").to(device)\n",
    "        out = vqa_model.generate(**inputs, max_length=20)\n",
    "        answer = vqa_processor.decode(out[0], skip_special_tokens=True)\n",
    "        print(f\"   Q: {question}\")\n",
    "        print(f\"   A: {answer}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "print(\"\\n‚úÖ Image analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Text Conversations with Mistral\n",
    "\n",
    "We'll use Mistral-7B-Instruct, a powerful open-source conversational AI model.\n",
    "\n",
    "**Note:** This model is large (~14GB). If Colab runs out of memory, we'll use a smaller model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Clear GPU memory\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "print(\"Loading conversational AI model...\\n\")\n",
    "\n",
    "try:\n",
    "    # Try loading Mistral-7B (better quality)\n",
    "    model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "    print(f\"Attempting to load {model_name}...\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "        device_map=\"auto\",\n",
    "        load_in_8bit=True if device == \"cuda\" else False\n",
    "    )\n",
    "    print(f\"‚úÖ {model_name} loaded successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    # Fallback to smaller model\n",
    "    print(f\"Could not load Mistral: {e}\")\n",
    "    print(\"\\nFalling back to smaller model...\")\n",
    "    model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    print(f\"‚úÖ {model_name} loaded successfully!\")\n",
    "\n",
    "print(f\"\\nModel: {model_name}\")\n",
    "print(\"Ready for conversation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-turn conversation\n",
    "conversation_history = []\n",
    "\n",
    "questions = [\n",
    "    \"Explain what multimodal AI is in simple terms.\",\n",
    "    \"What are some real-world applications of multimodal AI?\",\n",
    "    \"How does text-to-image generation like Stable Diffusion work?\",\n",
    "    \"What are the ethical concerns with AI-generated images?\",\n",
    "]\n",
    "\n",
    "print(\"Starting conversation...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, question in enumerate(questions):\n",
    "    print(f\"\\n[Turn {i+1}]\")\n",
    "    print(f\"User: {question}\")\n",
    "    \n",
    "    # Format the conversation for the model\n",
    "    if \"Mistral\" in model_name:\n",
    "        # Mistral format\n",
    "        messages = conversation_history + [{\"role\": \"user\", \"content\": question}]\n",
    "        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    else:\n",
    "        # TinyLlama format\n",
    "        prompt = f\"<|user|>\\n{question}</s>\\n<|assistant|>\\n\"\n",
    "    \n",
    "    # Generate response\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract just the assistant's response\n",
    "    if \"Mistral\" in model_name:\n",
    "        response = response.split(\"[/INST]\")[-1].strip()\n",
    "    else:\n",
    "        response = response.split(\"<|assistant|>\")[-1].strip()\n",
    "    \n",
    "    print(f\"\\nAI: {response}\")\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    \n",
    "    # Update conversation history\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": question})\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "print(\"\\n‚úÖ Conversation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Results\n",
    "\n",
    "This notebook demonstrated three key multimodal AI capabilities:\n",
    "\n",
    "### ‚úÖ Completed Tasks:\n",
    "\n",
    "1. **Text-to-Image Generation**\n",
    "   - Model: Stable Diffusion v1.5\n",
    "   - Generated 3 high-quality images from text prompts\n",
    "   - Demonstrated creative AI capabilities\n",
    "\n",
    "2. **Image Analysis**\n",
    "   - Model: BLIP (Salesforce)\n",
    "   - Analyzed generated images with captions and Q&A\n",
    "   - Showed understanding of visual content\n",
    "\n",
    "3. **Text Conversations**\n",
    "   - Model: Mistral-7B / TinyLlama\n",
    "   - Multi-turn conversation about AI topics\n",
    "   - Demonstrated context retention\n",
    "\n",
    "### üéØ Key Achievements:\n",
    "- **No API keys required** - all models run locally\n",
    "- **Free and open-source** - completely free to use\n",
    "- **State-of-the-art models** - Stable Diffusion, BLIP, Mistral\n",
    "- **Full multimodal pipeline** - text, images, and conversations\n",
    "\n",
    "### üìä Technical Details:\n",
    "- **Stable Diffusion**: 860M parameters, text-to-image generation\n",
    "- **BLIP**: 385M parameters, image understanding and captioning\n",
    "- **Mistral/TinyLlama**: 7B/1.1B parameters, conversational AI\n",
    "- **Hardware**: GPU-accelerated (T4/P100 on Colab)\n",
    "\n",
    "---\n",
    "\n",
    "### üìÅ Generated Files:\n",
    "- `generated_image_1.png` - Futuristic city scene\n",
    "- `generated_image_2.png` - Magical forest scene\n",
    "- `generated_image_3.png` - Robot chess scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all generated images in a grid\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "fig.suptitle('All Generated Images - Multimodal AI Demo', fontsize=16, fontweight='bold')\n",
    "\n",
    "for i, (ax, image, prompt) in enumerate(zip(axes, generated_images, prompts)):\n",
    "    ax.imshow(image)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f\"Image {i+1}\\n{prompt[:50]}...\", fontsize=9, wrap=True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ MULTIMODAL AI DEMO COMPLETE! üéâ\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n‚úÖ All tasks completed successfully:\")\n",
    "print(\"   1. Text-to-Image Generation ‚úì\")\n",
    "print(\"   2. Image Analysis ‚úì\")\n",
    "print(\"   3. Text Conversations ‚úì\")\n",
    "print(\"\\nüìä Total models used: 4 (Stable Diffusion + 2 BLIP + Mistral/TinyLlama)\")\n",
    "print(\"üí∞ Total cost: $0 (all free and open-source!)\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
