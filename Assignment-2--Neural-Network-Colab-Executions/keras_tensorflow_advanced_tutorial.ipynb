{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEkrSo01WcC7"
      },
      "source": [
        "# TensorFlow & Keras Advanced: Custom Layers and Deep Architectures\n",
        "\n",
        "## Part 2: Building Complex Operations from Scratch\n",
        "\n",
        "---\n",
        "\n",
        "In Part 1, we learned TensorFlow fundamentals, GradientTape basics, and the high-level Keras API. Now we go deeper!\n",
        "\n",
        "### What You'll Learn\n",
        "\n",
        "| Part | Topic | Key Concepts |\n",
        "|------|-------|-------------|\n",
        "| **I** | Advanced GradientTape | Nested tapes, Jacobians, custom gradients |\n",
        "| **II** | Building Ops from Scratch | Convolution, pooling, normalization by hand |\n",
        "| **III** | Custom Layers (Primitives) | Build layers using only tf.Variable |\n",
        "| **IV** | Custom Keras Layers | Proper subclassing with `build()` and `call()` |\n",
        "| **V** | Advanced Architectures | Residual blocks, attention, custom normalizations |\n",
        "| **VI** | Custom Training Loops | Full control over training with GradientTape |\n",
        "| **VII** | Practical Demos | Real-world examples with custom components |\n",
        "\n",
        "---\n",
        "\n",
        "*\"To understand the framework, build it from scratch.\"*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQaXbL0qWcC7",
        "outputId": "115f5fa7-35d6-44e3-d322-8be32b384dde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow Version: 2.19.0\n",
            "Keras Version:      3.10.0\n",
            "GPU Available:      False\n",
            "\n",
            "Ready for Advanced TensorFlow & Keras!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                           SETUP & IMPORTS\n",
        "# ============================================================================\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import layers, Model, Sequential\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Tuple, Optional, Callable, Union\n",
        "\n",
        "# Beautiful plots\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "# Check versions\n",
        "print(f\"TensorFlow Version: {tf.__version__}\")\n",
        "print(f\"Keras Version:      {keras.__version__}\")\n",
        "print(f\"GPU Available:      {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
        "\n",
        "# GPU memory growth\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    for gpu in gpus:\n",
        "        tf.config.experimental.set_memory_growth(gpu, True)\n",
        "\n",
        "# Reproducibility\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"\\nReady for Advanced TensorFlow & Keras!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tItbO-4WcC8"
      },
      "source": [
        "---\n",
        "\n",
        "# Part I: Advanced GradientTape Patterns\n",
        "\n",
        "## Beyond Basic Gradient Computation\n",
        "\n",
        "In Part 1, we used GradientTape for simple gradients. Now we'll explore:\n",
        "\n",
        "- **Nested tapes** for higher-order derivatives\n",
        "- **Jacobian and Hessian** computation\n",
        "- **Custom gradients** for non-differentiable operations\n",
        "- **Gradient clipping** and manipulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9-VhzBFWcC8",
        "outputId": "8823801a-2479-4668-e2d6-2f67e84da46a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "       NESTED TAPES: HIGHER-ORDER DERIVATIVES\n",
            "============================================================\n",
            "\n",
            "f(x) = x^4, evaluated at x = 2.0\n",
            "\n",
            "f(x)    = 16.0         (expected: 16)\n",
            "f'(x)   = 32.0         (expected: 32 = 4*8)\n",
            "f''(x)  = 48.0         (expected: 48 = 12*4)\n",
            "f'''(x) = 48.0         (expected: 48 = 24*2)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    NESTED GRADIENTTAPES: HIGHER-ORDER DERIVATIVES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"       NESTED TAPES: HIGHER-ORDER DERIVATIVES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Example: Compute first, second, and third derivatives\n",
        "# f(x) = x^4\n",
        "# f'(x) = 4x^3\n",
        "# f''(x) = 12x^2\n",
        "# f'''(x) = 24x\n",
        "\n",
        "x = tf.Variable(2.0)\n",
        "\n",
        "with tf.GradientTape() as tape3:\n",
        "    with tf.GradientTape() as tape2:\n",
        "        with tf.GradientTape() as tape1:\n",
        "            y = x ** 4\n",
        "        dy_dx = tape1.gradient(y, x)      # First derivative: 4x^3\n",
        "    d2y_dx2 = tape2.gradient(dy_dx, x)    # Second derivative: 12x^2\n",
        "d3y_dx3 = tape3.gradient(d2y_dx2, x)      # Third derivative: 24x\n",
        "\n",
        "print(f\"\\nf(x) = x^4, evaluated at x = {x.numpy()}\")\n",
        "print(f\"\")\n",
        "print(f\"f(x)    = {y.numpy():.1f}         (expected: 16)\")\n",
        "print(f\"f'(x)   = {dy_dx.numpy():.1f}         (expected: 32 = 4*8)\")\n",
        "print(f\"f''(x)  = {d2y_dx2.numpy():.1f}         (expected: 48 = 12*4)\")\n",
        "print(f\"f'''(x) = {d3y_dx3.numpy():.1f}         (expected: 48 = 24*2)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdSJanpUWcC8",
        "outputId": "584972e6-2bf1-48ba-b80c-4d666723fbe6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "              JACOBIAN COMPUTATION\n",
            "============================================================\n",
            "\n",
            "x = [1. 2. 3.]\n",
            "y = f(x) = [x1^2, x1*x2, sin(x3)] = [1.      2.      0.14112]\n",
            "\n",
            "Jacobian (3x3):\n",
            "[[ 2.         0.         0.       ]\n",
            " [ 2.         1.         0.       ]\n",
            " [ 0.         0.        -0.9899925]]\n",
            "\n",
            "Expected Jacobian:\n",
            "  [2*x1,   0,      0   ]   = [2,   0,     0     ]\n",
            "  [x2,     x1,     0   ]   = [2,   1,     0     ]\n",
            "  [0,      0,  cos(x3) ]   = [0,   0,  -0.9900]\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    JACOBIAN COMPUTATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"              JACOBIAN COMPUTATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# The Jacobian is the matrix of all first-order partial derivatives\n",
        "# For f: R^n -> R^m, the Jacobian J is m x n where J[i,j] = df_i/dx_j\n",
        "\n",
        "x = tf.Variable([1.0, 2.0, 3.0])\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    # Vector function: f(x) = [x1^2, x1*x2, sin(x3)]\n",
        "    y = tf.stack([\n",
        "        x[0] ** 2,\n",
        "        x[0] * x[1],\n",
        "        tf.sin(x[2])\n",
        "    ])\n",
        "\n",
        "# Compute full Jacobian\n",
        "jacobian = tape.jacobian(y, x)\n",
        "\n",
        "print(f\"\\nx = {x.numpy()}\")\n",
        "print(f\"y = f(x) = [x1^2, x1*x2, sin(x3)] = {y.numpy()}\")\n",
        "print(f\"\\nJacobian (3x3):\")\n",
        "print(f\"{jacobian.numpy()}\")\n",
        "\n",
        "print(f\"\\nExpected Jacobian:\")\n",
        "print(f\"  [2*x1,   0,      0   ]   = [2,   0,     0     ]\")\n",
        "print(f\"  [x2,     x1,     0   ]   = [2,   1,     0     ]\")\n",
        "print(f\"  [0,      0,  cos(x3) ]   = [0,   0,  {np.cos(3):.4f}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_a3dkRlWcC8",
        "outputId": "883a6a09-f876-4aa8-8ae9-e639cb7b0d5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "              HESSIAN COMPUTATION\n",
            "============================================================\n",
            "\n",
            "f(x, y) = x^2*y + y^3, at (x, y) = (1.0, 2.0)\n",
            "f = 10.0\n",
            "\n",
            "Gradient: [ 4. 13.]\n",
            "  Expected: [2xy, x^2 + 3y^2] = [4, 13]\n",
            "\n",
            "Hessian:\n",
            "[[ 4.  2.]\n",
            " [ 2. 12.]]\n",
            "  Expected:\n",
            "  [2y,  2x ]   = [4, 2]\n",
            "  [2x,  6y ]   = [2, 12]\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    HESSIAN COMPUTATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"              HESSIAN COMPUTATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# The Hessian is the matrix of second-order partial derivatives\n",
        "# H[i,j] = d^2f / (dx_i dx_j)\n",
        "\n",
        "x = tf.Variable([1.0, 2.0])\n",
        "\n",
        "with tf.GradientTape() as tape2:\n",
        "    with tf.GradientTape() as tape1:\n",
        "        # Scalar function: f(x, y) = x^2*y + y^3\n",
        "        f = x[0]**2 * x[1] + x[1]**3\n",
        "    grad = tape1.gradient(f, x)  # [2xy, x^2 + 3y^2]\n",
        "hessian = tape2.jacobian(grad, x)\n",
        "\n",
        "print(f\"\\nf(x, y) = x^2*y + y^3, at (x, y) = ({x[0].numpy()}, {x[1].numpy()})\")\n",
        "print(f\"f = {f.numpy()}\")\n",
        "print(f\"\\nGradient: {grad.numpy()}\")\n",
        "print(f\"  Expected: [2xy, x^2 + 3y^2] = [4, 13]\")\n",
        "print(f\"\\nHessian:\")\n",
        "print(f\"{hessian.numpy()}\")\n",
        "print(f\"  Expected:\")\n",
        "print(f\"  [2y,  2x ]   = [4, 2]\")\n",
        "print(f\"  [2x,  6y ]   = [2, 12]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srxJFP_KWcC9",
        "outputId": "dc7d880d-bf52-4af2-f16f-34d262338ec1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "              CUSTOM GRADIENTS\n",
            "============================================================\n",
            "\n",
            "Input: [3. 4.]\n",
            "Gradient (clipped to norm 1.0): [0.70710677 0.70710677]\n",
            "Gradient norm: 1.0000\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    CUSTOM GRADIENTS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"              CUSTOM GRADIENTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Sometimes you need to define custom gradients:\n",
        "# - For non-differentiable operations (like argmax)\n",
        "# - For numerical stability\n",
        "# - For custom backward passes (like straight-through estimators)\n",
        "\n",
        "@tf.custom_gradient\n",
        "def clip_gradient_norm(x, clip_value=1.0):\n",
        "    \"\"\"\n",
        "    Forward: identity function\n",
        "    Backward: clip gradient norm\n",
        "    \"\"\"\n",
        "    def grad(dy):\n",
        "        # Clip the incoming gradient\n",
        "        norm = tf.norm(dy)\n",
        "        clipped = tf.cond(\n",
        "            norm > clip_value,\n",
        "            lambda: dy * clip_value / norm,\n",
        "            lambda: dy\n",
        "        )\n",
        "        return clipped  # Only return gradient for 'x'\n",
        "    return x, grad\n",
        "\n",
        "# Test custom gradient\n",
        "x = tf.Variable([3.0, 4.0])  # Gradient will have norm 5 (3-4-5 triangle)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    y = clip_gradient_norm(x, clip_value=1.0)\n",
        "    loss = tf.reduce_sum(y)  # Gradient would be [1, 1] but we pass [3, 4]\n",
        "\n",
        "# Manually set upstream gradient to [3, 4]\n",
        "grad = tape.gradient(loss, x)\n",
        "print(f\"\\nInput: {x.numpy()}\")\n",
        "print(f\"Gradient (clipped to norm 1.0): {grad.numpy()}\")\n",
        "print(f\"Gradient norm: {tf.norm(grad).numpy():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hl8-8EpEWcC9",
        "outputId": "1da26899-954e-46af-dc95-2adbecbd19c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "         STRAIGHT-THROUGH ESTIMATOR\n",
            "============================================================\n",
            "\n",
            "Input:   [0.3 0.7 1.2 2.5]\n",
            "Rounded: [0. 1. 1. 2.]\n",
            "Gradient (straight-through): [0. 2. 2. 4.]\n",
            "\n",
            " Note: Round is non-differentiable, but we can still train!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    STRAIGHT-THROUGH ESTIMATOR\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"         STRAIGHT-THROUGH ESTIMATOR\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# The straight-through estimator is used for:\n",
        "# - Binary/discrete operations that are non-differentiable\n",
        "# - Quantization in neural networks\n",
        "\n",
        "@tf.custom_gradient\n",
        "def straight_through_round(x):\n",
        "    \"\"\"\n",
        "    Forward: round to nearest integer\n",
        "    Backward: pass gradient through unchanged (identity)\n",
        "    \"\"\"\n",
        "    def grad(dy):\n",
        "        return dy  # Straight-through: gradient = identity\n",
        "    return tf.round(x), grad\n",
        "\n",
        "@tf.custom_gradient\n",
        "def straight_through_sign(x):\n",
        "    \"\"\"\n",
        "    Forward: sign function (-1, 0, or 1)\n",
        "    Backward: gradient of hard tanh (1 if |x| <= 1, else 0)\n",
        "    \"\"\"\n",
        "    def grad(dy):\n",
        "        # Gradient is 1 where |x| <= 1, 0 elsewhere\n",
        "        return dy * tf.cast(tf.abs(x) <= 1, dy.dtype)\n",
        "    return tf.sign(x), grad\n",
        "\n",
        "# Test\n",
        "x = tf.Variable([0.3, 0.7, 1.2, 2.5])\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    y = straight_through_round(x)\n",
        "    loss = tf.reduce_sum(y ** 2)\n",
        "\n",
        "grad = tape.gradient(loss, x)\n",
        "\n",
        "print(f\"\\nInput:   {x.numpy()}\")\n",
        "print(f\"Rounded: {y.numpy()}\")\n",
        "print(f\"Gradient (straight-through): {grad.numpy()}\")\n",
        "print(f\"\\n Note: Round is non-differentiable, but we can still train!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Uwxn2-LWcC9",
        "outputId": "e1c1381a-7af8-4960-96de-5cd73210fc64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "            GRADIENT ACCUMULATION\n",
            "============================================================\n",
            "\n",
            "Gradient Accumulation Pattern:\n",
            "  1. Compute gradients for mini-batch\n",
            "  2. Accumulate (sum or average) over N steps\n",
            "  3. Apply accumulated gradients once\n",
            "  4. Effective batch = mini_batch * N\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    GRADIENT ACCUMULATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"            GRADIENT ACCUMULATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Gradient accumulation is useful when:\n",
        "# - Batch size is too large for GPU memory\n",
        "# - You want effective larger batch sizes\n",
        "\n",
        "def train_with_accumulation(model, data, labels, batch_size, accumulation_steps, optimizer):\n",
        "    \"\"\"\n",
        "    Train with gradient accumulation.\n",
        "    Effective batch size = batch_size * accumulation_steps\n",
        "    \"\"\"\n",
        "    n_samples = len(data)\n",
        "    accumulated_gradients = [tf.zeros_like(v) for v in model.trainable_variables]\n",
        "\n",
        "    for step in range(accumulation_steps):\n",
        "        # Get mini-batch\n",
        "        start = (step * batch_size) % n_samples\n",
        "        end = start + batch_size\n",
        "        x_batch = data[start:end]\n",
        "        y_batch = labels[start:end]\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = model(x_batch, training=True)\n",
        "            loss = tf.reduce_mean(keras.losses.mse(y_batch, predictions))\n",
        "\n",
        "        # Compute gradients\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "        # Accumulate (average over steps)\n",
        "        accumulated_gradients = [\n",
        "            acc + grad / accumulation_steps\n",
        "            for acc, grad in zip(accumulated_gradients, gradients)\n",
        "        ]\n",
        "\n",
        "    # Apply accumulated gradients\n",
        "    optimizer.apply_gradients(zip(accumulated_gradients, model.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "print(\"\\nGradient Accumulation Pattern:\")\n",
        "print(\"  1. Compute gradients for mini-batch\")\n",
        "print(\"  2. Accumulate (sum or average) over N steps\")\n",
        "print(\"  3. Apply accumulated gradients once\")\n",
        "print(\"  4. Effective batch = mini_batch * N\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WCMnLwcWcC9"
      },
      "source": [
        "---\n",
        "\n",
        "# Part II: Building Operations from Scratch\n",
        "\n",
        "## Understanding Neural Network Primitives\n",
        "\n",
        "Before using Keras layers, let's understand what they do by building them ourselves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PY-nVGvvWcC-",
        "outputId": "9f1ebbc7-ea45-4dff-c7b5-0f9c2505ca0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "           CONVOLUTION FROM SCRATCH\n",
            "============================================================\n",
            "\n",
            "Input shape:  (1, 5, 5, 1)\n",
            "Kernel shape: (3, 3, 1, 2)\n",
            "Output shape: (1, 3, 3, 2)\n",
            "Matches tf.nn.conv2d: True\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    CONVOLUTION FROM SCRATCH\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"           CONVOLUTION FROM SCRATCH\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def conv2d_naive(input_tensor, kernel, stride=1, padding='VALID'):\n",
        "    \"\"\"\n",
        "    Naive 2D convolution implementation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_tensor : tensor (batch, height, width, in_channels)\n",
        "    kernel : tensor (kernel_h, kernel_w, in_channels, out_channels)\n",
        "    stride : int\n",
        "    padding : 'VALID' or 'SAME'\n",
        "    \"\"\"\n",
        "    batch_size = tf.shape(input_tensor)[0]\n",
        "    in_h, in_w = input_tensor.shape[1], input_tensor.shape[2]\n",
        "    k_h, k_w = kernel.shape[0], kernel.shape[1]\n",
        "    out_channels = kernel.shape[3]\n",
        "\n",
        "    if padding == 'SAME':\n",
        "        pad_h = k_h // 2\n",
        "        pad_w = k_w // 2\n",
        "        input_tensor = tf.pad(input_tensor,\n",
        "                              [[0, 0], [pad_h, pad_h], [pad_w, pad_w], [0, 0]])\n",
        "        in_h += 2 * pad_h\n",
        "        in_w += 2 * pad_w\n",
        "\n",
        "    out_h = (in_h - k_h) // stride + 1\n",
        "    out_w = (in_w - k_w) // stride + 1\n",
        "\n",
        "    output = tf.TensorArray(dtype=tf.float32, size=out_h * out_w)\n",
        "    idx = 0\n",
        "\n",
        "    for i in range(out_h):\n",
        "        for j in range(out_w):\n",
        "            # Extract patch\n",
        "            h_start = i * stride\n",
        "            w_start = j * stride\n",
        "            patch = input_tensor[:, h_start:h_start+k_h, w_start:w_start+k_w, :]\n",
        "\n",
        "            # Convolve: sum over (h, w, in_channels), keep out_channels\n",
        "            # patch: (batch, k_h, k_w, in_c)\n",
        "            # kernel: (k_h, k_w, in_c, out_c)\n",
        "            conv = tf.einsum('bhwi,hwio->bo', patch, kernel)\n",
        "            output = output.write(idx, conv)\n",
        "            idx += 1\n",
        "\n",
        "    output = output.stack()  # (out_h*out_w, batch, out_c)\n",
        "    output = tf.transpose(output, [1, 0, 2])  # (batch, out_h*out_w, out_c)\n",
        "    output = tf.reshape(output, [batch_size, out_h, out_w, out_channels])\n",
        "\n",
        "    return output\n",
        "\n",
        "# Test our implementation\n",
        "x = tf.random.normal((1, 5, 5, 1))  # 1 image, 5x5, 1 channel\n",
        "kernel = tf.random.normal((3, 3, 1, 2))  # 3x3 kernel, 1->2 channels\n",
        "\n",
        "our_output = conv2d_naive(x, kernel, stride=1, padding='VALID')\n",
        "tf_output = tf.nn.conv2d(x, kernel, strides=1, padding='VALID')\n",
        "\n",
        "print(f\"\\nInput shape:  {x.shape}\")\n",
        "print(f\"Kernel shape: {kernel.shape}\")\n",
        "print(f\"Output shape: {our_output.shape}\")\n",
        "print(f\"Matches tf.nn.conv2d: {tf.reduce_all(tf.abs(our_output - tf_output) < 1e-5).numpy()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltjjl1lJWcC-",
        "outputId": "b3fc7cb7-7bc0-4ee1-d82b-8b9f054a262b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "           MAX POOLING FROM SCRATCH\n",
            "============================================================\n",
            "\n",
            "Input shape: (1, 4, 4, 2)\n",
            "Input (channel 0):\n",
            "[[ 1.  3.  5.  7.]\n",
            " [ 9. 11. 13. 15.]\n",
            " [17. 19. 21. 23.]\n",
            " [25. 27. 29. 31.]]\n",
            "\n",
            "Output shape: (1, 2, 2, 2)\n",
            "Output (channel 0):\n",
            "[[11. 15.]\n",
            " [27. 31.]]\n",
            "\n",
            "Matches tf.nn.max_pool2d: True\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    MAX POOLING FROM SCRATCH\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"           MAX POOLING FROM SCRATCH\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def max_pool2d_naive(input_tensor, pool_size=2, stride=2):\n",
        "    \"\"\"\n",
        "    Naive max pooling implementation.\n",
        "\n",
        "    For each pool_size x pool_size window, take the maximum.\n",
        "    \"\"\"\n",
        "    batch_size = tf.shape(input_tensor)[0]\n",
        "    in_h, in_w, channels = input_tensor.shape[1:]\n",
        "\n",
        "    out_h = (in_h - pool_size) // stride + 1\n",
        "    out_w = (in_w - pool_size) // stride + 1\n",
        "\n",
        "    outputs = []\n",
        "\n",
        "    for i in range(out_h):\n",
        "        row = []\n",
        "        for j in range(out_w):\n",
        "            h_start = i * stride\n",
        "            w_start = j * stride\n",
        "            # Extract window\n",
        "            window = input_tensor[:, h_start:h_start+pool_size,\n",
        "                                  w_start:w_start+pool_size, :]\n",
        "            # Max over spatial dimensions\n",
        "            pooled = tf.reduce_max(window, axis=[1, 2])\n",
        "            row.append(pooled)\n",
        "        outputs.append(tf.stack(row, axis=1))\n",
        "\n",
        "    return tf.stack(outputs, axis=1)\n",
        "\n",
        "# Test\n",
        "x = tf.constant([[[[1., 2.], [3., 4.], [5., 6.], [7., 8.]],\n",
        "                  [[9., 10.], [11., 12.], [13., 14.], [15., 16.]],\n",
        "                  [[17., 18.], [19., 20.], [21., 22.], [23., 24.]],\n",
        "                  [[25., 26.], [27., 28.], [29., 30.], [31., 32.]]]])\n",
        "\n",
        "print(f\"\\nInput shape: {x.shape}\")\n",
        "print(f\"Input (channel 0):\")\n",
        "print(x[0, :, :, 0].numpy())\n",
        "\n",
        "our_pool = max_pool2d_naive(x, pool_size=2, stride=2)\n",
        "tf_pool = tf.nn.max_pool2d(x, ksize=2, strides=2, padding='VALID')\n",
        "\n",
        "print(f\"\\nOutput shape: {our_pool.shape}\")\n",
        "print(f\"Output (channel 0):\")\n",
        "print(our_pool[0, :, :, 0].numpy())\n",
        "print(f\"\\nMatches tf.nn.max_pool2d: {tf.reduce_all(our_pool == tf_pool).numpy()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeZuo5ITWcC-",
        "outputId": "2cccd3ee-7ecd-4154-eea7-98beaf361100"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "        BATCH NORMALIZATION FROM SCRATCH\n",
            "============================================================\n",
            "\n",
            "Input shape: (8, 4)\n",
            "Input mean per feature: [ 0.32574826 -0.22160122  0.37827352  0.3575499 ]\n",
            "Input std per feature:  [0.8102391  0.74234474 1.061351   1.0821929 ]\n",
            "\n",
            "Output (training) mean: [-2.9802322e-08  4.6566129e-08  7.4505806e-09 -2.2351742e-08]\n",
            "Output (training) std:  [0.9999923 0.9999908 0.9999955 0.9999958]\n",
            "\n",
            " After BatchNorm, each feature has ~0 mean and ~1 std!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    BATCH NORMALIZATION FROM SCRATCH\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"        BATCH NORMALIZATION FROM SCRATCH\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class BatchNormFromScratch:\n",
        "    \"\"\"\n",
        "    Batch Normalization implemented from scratch.\n",
        "\n",
        "    During training:\n",
        "        x_norm = (x - batch_mean) / sqrt(batch_var + epsilon)\n",
        "        y = gamma * x_norm + beta\n",
        "\n",
        "    During inference:\n",
        "        Use running mean and variance instead of batch statistics.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_features, epsilon=1e-5, momentum=0.1):\n",
        "        self.epsilon = epsilon\n",
        "        self.momentum = momentum\n",
        "\n",
        "        # Learnable parameters\n",
        "        self.gamma = tf.Variable(tf.ones(num_features), name='gamma')\n",
        "        self.beta = tf.Variable(tf.zeros(num_features), name='beta')\n",
        "\n",
        "        # Running statistics (not trainable)\n",
        "        self.running_mean = tf.Variable(tf.zeros(num_features), trainable=False)\n",
        "        self.running_var = tf.Variable(tf.ones(num_features), trainable=False)\n",
        "\n",
        "    def __call__(self, x, training=True):\n",
        "        if training:\n",
        "            # Compute batch statistics\n",
        "            batch_mean = tf.reduce_mean(x, axis=0)\n",
        "            batch_var = tf.math.reduce_variance(x, axis=0)\n",
        "\n",
        "            # Update running statistics\n",
        "            self.running_mean.assign(\n",
        "                (1 - self.momentum) * self.running_mean + self.momentum * batch_mean\n",
        "            )\n",
        "            self.running_var.assign(\n",
        "                (1 - self.momentum) * self.running_var + self.momentum * batch_var\n",
        "            )\n",
        "\n",
        "            mean, var = batch_mean, batch_var\n",
        "        else:\n",
        "            mean, var = self.running_mean, self.running_var\n",
        "\n",
        "        # Normalize\n",
        "        x_norm = (x - mean) / tf.sqrt(var + self.epsilon)\n",
        "\n",
        "        # Scale and shift\n",
        "        return self.gamma * x_norm + self.beta\n",
        "\n",
        "    @property\n",
        "    def trainable_variables(self):\n",
        "        return [self.gamma, self.beta]\n",
        "\n",
        "# Test\n",
        "bn = BatchNormFromScratch(num_features=4)\n",
        "x = tf.random.normal((8, 4))  # Batch of 8, 4 features\n",
        "\n",
        "y_train = bn(x, training=True)\n",
        "y_eval = bn(x, training=False)\n",
        "\n",
        "print(f\"\\nInput shape: {x.shape}\")\n",
        "print(f\"Input mean per feature: {tf.reduce_mean(x, axis=0).numpy()}\")\n",
        "print(f\"Input std per feature:  {tf.math.reduce_std(x, axis=0).numpy()}\")\n",
        "print(f\"\\nOutput (training) mean: {tf.reduce_mean(y_train, axis=0).numpy()}\")\n",
        "print(f\"Output (training) std:  {tf.math.reduce_std(y_train, axis=0).numpy()}\")\n",
        "print(f\"\\n After BatchNorm, each feature has ~0 mean and ~1 std!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YR_REX3RWcC-",
        "outputId": "8a5adec1-113f-42ff-f009-037c667446b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "        LAYER NORMALIZATION FROM SCRATCH\n",
            "============================================================\n",
            "\n",
            "Input shape: (2, 3, 4)\n",
            "\n",
            "For sample [0, 0, :]:\n",
            "  Input:  [ 0.65648675 -0.4130517   0.33997506 -1.0056272 ]\n",
            "  Output: [ 1.1744822  -0.47392502  0.6866642  -1.3872216 ]\n",
            "  Output mean: -0.000000\n",
            "  Output std:  1.0000\n",
            "\n",
            " Key difference:\n",
            "  BatchNorm: normalize across batch (for each feature)\n",
            "  LayerNorm: normalize across features (for each sample)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    LAYER NORMALIZATION FROM SCRATCH\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"        LAYER NORMALIZATION FROM SCRATCH\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class LayerNormFromScratch:\n",
        "    \"\"\"\n",
        "    Layer Normalization: Normalize across features (not batch).\n",
        "\n",
        "    Used in Transformers because:\n",
        "    - Works with any batch size (including 1)\n",
        "    - No running statistics needed\n",
        "    - Each sample normalized independently\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, normalized_shape, epsilon=1e-5):\n",
        "        self.epsilon = epsilon\n",
        "        self.normalized_shape = normalized_shape\n",
        "\n",
        "        # Learnable parameters\n",
        "        self.gamma = tf.Variable(tf.ones(normalized_shape), name='gamma')\n",
        "        self.beta = tf.Variable(tf.zeros(normalized_shape), name='beta')\n",
        "\n",
        "    def __call__(self, x):\n",
        "        # Compute statistics across last dimensions\n",
        "        mean = tf.reduce_mean(x, axis=-1, keepdims=True)\n",
        "        var = tf.math.reduce_variance(x, axis=-1, keepdims=True)\n",
        "\n",
        "        # Normalize\n",
        "        x_norm = (x - mean) / tf.sqrt(var + self.epsilon)\n",
        "\n",
        "        # Scale and shift\n",
        "        return self.gamma * x_norm + self.beta\n",
        "\n",
        "    @property\n",
        "    def trainable_variables(self):\n",
        "        return [self.gamma, self.beta]\n",
        "\n",
        "# Test\n",
        "ln = LayerNormFromScratch(normalized_shape=4)\n",
        "x = tf.random.normal((2, 3, 4))  # (batch, seq, features)\n",
        "\n",
        "y = ln(x)\n",
        "\n",
        "print(f\"\\nInput shape: {x.shape}\")\n",
        "print(f\"\\nFor sample [0, 0, :]:\")\n",
        "print(f\"  Input:  {x[0, 0, :].numpy()}\")\n",
        "print(f\"  Output: {y[0, 0, :].numpy()}\")\n",
        "print(f\"  Output mean: {tf.reduce_mean(y[0, 0, :]).numpy():.6f}\")\n",
        "print(f\"  Output std:  {tf.math.reduce_std(y[0, 0, :]).numpy():.4f}\")\n",
        "\n",
        "print(f\"\\n Key difference:\")\n",
        "print(f\"  BatchNorm: normalize across batch (for each feature)\")\n",
        "print(f\"  LayerNorm: normalize across features (for each sample)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4Ok8o68WcC_",
        "outputId": "33456f99-1645-4f83-e98d-8be98a9157dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "            DROPOUT FROM SCRATCH\n",
            "============================================================\n",
            "Input: all ones, shape (2, 10)\n",
            "\n",
            "Dropout sample 1: [0. 2. 2. 0. 0. 2. 0. 2. 2. 0.]\n",
            "Dropout sample 2: [0. 2. 2. 2. 0. 0. 2. 2. 2. 0.]\n",
            "Dropout sample 3: [2. 0. 2. 2. 2. 0. 0. 2. 2. 2.]\n",
            "\n",
            "During inference (training=False):\n",
            "  Output: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "\n",
            "Average over 1000 samples: 1.0045 (should be ~1.0)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    DROPOUT FROM SCRATCH\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"            DROPOUT FROM SCRATCH\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def dropout_from_scratch(x, rate=0.5, training=True):\n",
        "    \"\"\"\n",
        "    Dropout: Randomly zero out neurons during training.\n",
        "\n",
        "    Key insight: Scale by 1/(1-rate) during training so that\n",
        "    expected value remains the same during inference.\n",
        "    \"\"\"\n",
        "    if not training or rate == 0:\n",
        "        return x\n",
        "\n",
        "    # Create random mask\n",
        "    keep_prob = 1 - rate\n",
        "    mask = tf.cast(\n",
        "        tf.random.uniform(tf.shape(x)) < keep_prob,\n",
        "        dtype=x.dtype\n",
        "    )\n",
        "\n",
        "    # Apply mask and scale\n",
        "    return (x * mask) / keep_prob\n",
        "\n",
        "# Test\n",
        "x = tf.ones((2, 10))\n",
        "\n",
        "print(f\"Input: all ones, shape {x.shape}\")\n",
        "print(f\"\")\n",
        "\n",
        "# Multiple dropout samples\n",
        "for i in range(3):\n",
        "    dropped = dropout_from_scratch(x, rate=0.5, training=True)\n",
        "    print(f\"Dropout sample {i+1}: {dropped[0].numpy()}\")\n",
        "\n",
        "print(f\"\\nDuring inference (training=False):\")\n",
        "print(f\"  Output: {dropout_from_scratch(x, rate=0.5, training=False)[0].numpy()}\")\n",
        "\n",
        "# Verify expected value is preserved\n",
        "samples = tf.stack([dropout_from_scratch(x, rate=0.5) for _ in range(1000)])\n",
        "print(f\"\\nAverage over 1000 samples: {tf.reduce_mean(samples).numpy():.4f} (should be ~1.0)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Part III: Custom Layers Using Primitives\n",
        "\n",
        "## Building Layers with tf.Variable Only\n",
        "\n",
        "Before using Keras's layer system, let's build fully functional layers using only basic TensorFlow operations. This shows exactly what happens under the hood."
      ],
      "metadata": {
        "id": "4XqEZxTtWcC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    DENSE LAYER FROM PRIMITIVES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"          DENSE LAYER FROM PRIMITIVES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class DenseLayerPrimitive:\n",
        "    \"\"\"\n",
        "    Fully connected layer using only tf.Variable.\n",
        "\n",
        "    Mathematically: y = activation(x @ W + b)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, activation=None, use_bias=True):\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "        # Select activation\n",
        "        self.activation = {\n",
        "            None: lambda x: x,\n",
        "            'relu': tf.nn.relu,\n",
        "            'sigmoid': tf.nn.sigmoid,\n",
        "            'tanh': tf.nn.tanh,\n",
        "            'softmax': lambda x: tf.nn.softmax(x, axis=-1)\n",
        "        }.get(activation, activation)  # Allow passing functions directly\n",
        "\n",
        "        # He initialization for weights\n",
        "        stddev = np.sqrt(2.0 / in_features)\n",
        "        self.W = tf.Variable(\n",
        "            tf.random.normal((in_features, out_features), stddev=stddev),\n",
        "            trainable=True,\n",
        "            name='kernel'\n",
        "        )\n",
        "\n",
        "        if use_bias:\n",
        "            self.b = tf.Variable(\n",
        "                tf.zeros(out_features),\n",
        "                trainable=True,\n",
        "                name='bias'\n",
        "            )\n",
        "\n",
        "    def __call__(self, x):\n",
        "        \"\"\"Forward pass: y = activation(x @ W + b)\"\"\"\n",
        "        out = x @ self.W\n",
        "        if self.use_bias:\n",
        "            out = out + self.b\n",
        "        return self.activation(out)\n",
        "\n",
        "    @property\n",
        "    def trainable_variables(self):\n",
        "        if self.use_bias:\n",
        "            return [self.W, self.b]\n",
        "        return [self.W]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"DenseLayerPrimitive({self.in_features}, {self.out_features})\"\n",
        "\n",
        "# Test\n",
        "dense = DenseLayerPrimitive(4, 3, activation='relu')\n",
        "x = tf.random.normal((2, 4))\n",
        "y = dense(x)\n",
        "\n",
        "print(f\"\\nDenseLayerPrimitive(4, 3, activation='relu')\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"Weight shape: {dense.W.shape}\")\n",
        "print(f\"Bias shape:   {dense.b.shape}\")\n",
        "print(f\"\\nOutput:\\n{y.numpy()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gesc0NBxWcC_",
        "outputId": "83c3e0bf-3bd1-4b55-9e9b-8a33b554a4cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "          DENSE LAYER FROM PRIMITIVES\n",
            "============================================================\n",
            "\n",
            "DenseLayerPrimitive(4, 3, activation='relu')\n",
            "Input shape:  (2, 4)\n",
            "Output shape: (2, 3)\n",
            "Weight shape: (4, 3)\n",
            "Bias shape:   (3,)\n",
            "\n",
            "Output:\n",
            "[[0.         0.         0.46504724]\n",
            " [0.         0.         0.8249154 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    CONV2D LAYER FROM PRIMITIVES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"          CONV2D LAYER FROM PRIMITIVES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class Conv2DLayerPrimitive:\n",
        "    \"\"\"\n",
        "    2D Convolutional layer using only tf.Variable and tf.nn.conv2d.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size,\n",
        "                 stride=1, padding='SAME', activation=None, use_bias=True):\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "        # Handle kernel_size as int or tuple\n",
        "        if isinstance(kernel_size, int):\n",
        "            kernel_size = (kernel_size, kernel_size)\n",
        "\n",
        "        # Select activation\n",
        "        self.activation = {\n",
        "            None: lambda x: x,\n",
        "            'relu': tf.nn.relu,\n",
        "            'sigmoid': tf.nn.sigmoid,\n",
        "            'tanh': tf.nn.tanh,\n",
        "        }.get(activation, activation)\n",
        "\n",
        "        # He initialization\n",
        "        fan_in = kernel_size[0] * kernel_size[1] * in_channels\n",
        "        stddev = np.sqrt(2.0 / fan_in)\n",
        "\n",
        "        # Kernel shape: (height, width, in_channels, out_channels)\n",
        "        self.kernel = tf.Variable(\n",
        "            tf.random.normal((kernel_size[0], kernel_size[1], in_channels, out_channels),\n",
        "                           stddev=stddev),\n",
        "            trainable=True,\n",
        "            name='kernel'\n",
        "        )\n",
        "\n",
        "        if use_bias:\n",
        "            self.bias = tf.Variable(\n",
        "                tf.zeros(out_channels),\n",
        "                trainable=True,\n",
        "                name='bias'\n",
        "            )\n",
        "\n",
        "    def __call__(self, x):\n",
        "        \"\"\"Forward pass using tf.nn.conv2d\"\"\"\n",
        "        out = tf.nn.conv2d(x, self.kernel, strides=self.stride, padding=self.padding)\n",
        "        if self.use_bias:\n",
        "            out = out + self.bias\n",
        "        return self.activation(out)\n",
        "\n",
        "    @property\n",
        "    def trainable_variables(self):\n",
        "        if self.use_bias:\n",
        "            return [self.kernel, self.bias]\n",
        "        return [self.kernel]\n",
        "\n",
        "# Test\n",
        "conv = Conv2DLayerPrimitive(3, 16, kernel_size=3, activation='relu')\n",
        "x = tf.random.normal((1, 28, 28, 3))  # 1 image, 28x28, 3 channels\n",
        "y = conv(x)\n",
        "\n",
        "print(f\"\\nConv2DLayerPrimitive(3, 16, kernel_size=3)\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"Kernel shape: {conv.kernel.shape}\")\n",
        "print(f\"Parameters:   {np.prod(conv.kernel.shape) + conv.bias.shape[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSWj-SXwWcC_",
        "outputId": "6bec0d31-4c5f-48d1-b0a2-f2dd194abad1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "          CONV2D LAYER FROM PRIMITIVES\n",
            "============================================================\n",
            "\n",
            "Conv2DLayerPrimitive(3, 16, kernel_size=3)\n",
            "Input shape:  (1, 28, 28, 3)\n",
            "Output shape: (1, 28, 28, 16)\n",
            "Kernel shape: (3, 3, 3, 16)\n",
            "Parameters:   448\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    COMPLETE CNN FROM PRIMITIVES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"          COMPLETE CNN FROM PRIMITIVES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class CNNFromPrimitives:\n",
        "    \"\"\"\n",
        "    A complete CNN built using only primitive layers.\n",
        "\n",
        "    Architecture:\n",
        "        Conv(3x3) -> ReLU -> MaxPool -> Conv(3x3) -> ReLU -> MaxPool -> Flatten -> Dense -> Dense\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_shape, num_classes):\n",
        "        self.input_shape = input_shape\n",
        "        in_channels = input_shape[-1]\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.conv1 = Conv2DLayerPrimitive(in_channels, 32, kernel_size=3, activation='relu')\n",
        "        self.conv2 = Conv2DLayerPrimitive(32, 64, kernel_size=3, activation='relu')\n",
        "\n",
        "        # Calculate flattened size after convolutions and pooling\n",
        "        # With SAME padding and 2x2 pooling twice: H/4, W/4\n",
        "        h, w = input_shape[0] // 4, input_shape[1] // 4\n",
        "        flat_size = h * w * 64\n",
        "\n",
        "        # Dense layers\n",
        "        self.fc1 = DenseLayerPrimitive(flat_size, 128, activation='relu')\n",
        "        self.fc2 = DenseLayerPrimitive(128, num_classes, activation='softmax')\n",
        "\n",
        "        self.layers = [self.conv1, self.conv2, self.fc1, self.fc2]\n",
        "\n",
        "    def __call__(self, x, training=True):\n",
        "        # Conv block 1\n",
        "        x = self.conv1(x)\n",
        "        x = tf.nn.max_pool2d(x, ksize=2, strides=2, padding='SAME')\n",
        "\n",
        "        # Conv block 2\n",
        "        x = self.conv2(x)\n",
        "        x = tf.nn.max_pool2d(x, ksize=2, strides=2, padding='SAME')\n",
        "\n",
        "        # Flatten\n",
        "        x = tf.reshape(x, (tf.shape(x)[0], -1))\n",
        "\n",
        "        # Dense layers\n",
        "        x = self.fc1(x)\n",
        "        if training:\n",
        "            x = dropout_from_scratch(x, rate=0.5, training=True)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    @property\n",
        "    def trainable_variables(self):\n",
        "        variables = []\n",
        "        for layer in self.layers:\n",
        "            variables.extend(layer.trainable_variables)\n",
        "        return variables\n",
        "\n",
        "    def summary(self):\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"        CNN FROM PRIMITIVES - SUMMARY\")\n",
        "        print(\"=\"*50)\n",
        "        total = 0\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            params = sum(np.prod(v.shape) for v in layer.trainable_variables)\n",
        "            total += params\n",
        "            print(f\"Layer {i+1}: {layer.__class__.__name__:20} | Params: {params:,}\")\n",
        "        print(\"-\"*50)\n",
        "        print(f\"Total trainable parameters: {total:,}\")\n",
        "\n",
        "# Create and test\n",
        "cnn = CNNFromPrimitives(input_shape=(28, 28, 1), num_classes=10)\n",
        "cnn.summary()\n",
        "\n",
        "# Test forward pass\n",
        "x = tf.random.normal((4, 28, 28, 1))\n",
        "y = cnn(x)\n",
        "print(f\"\\nInput shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"Output sum per sample: {tf.reduce_sum(y, axis=1).numpy()}  (should be ~1.0)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvCl_LZwWcC_",
        "outputId": "39097caf-a6fb-473d-e9c7-381f136cad4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "          COMPLETE CNN FROM PRIMITIVES\n",
            "============================================================\n",
            "\n",
            "==================================================\n",
            "        CNN FROM PRIMITIVES - SUMMARY\n",
            "==================================================\n",
            "Layer 1: Conv2DLayerPrimitive | Params: 320\n",
            "Layer 2: Conv2DLayerPrimitive | Params: 18,496\n",
            "Layer 3: DenseLayerPrimitive  | Params: 401,536\n",
            "Layer 4: DenseLayerPrimitive  | Params: 1,290\n",
            "--------------------------------------------------\n",
            "Total trainable parameters: 421,642\n",
            "\n",
            "Input shape:  (4, 28, 28, 1)\n",
            "Output shape: (4, 10)\n",
            "Output sum per sample: [0.9999999 1.        1.        0.9999999]  (should be ~1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Part IV: Custom Keras Layers\n",
        "\n",
        "## The Proper Way to Build Custom Layers\n",
        "\n",
        "Keras provides a clean API for custom layers with:\n",
        "- **`build()`**: Create weights when input shape is known\n",
        "- **`call()`**: Define the forward pass\n",
        "- **`get_config()`**: Enable serialization\n",
        "\n",
        "This gives you all the benefits of primitives PLUS:\n",
        "- Automatic weight tracking\n",
        "- Serialization/deserialization\n",
        "- Integration with `model.fit()`\n",
        "- Proper shape inference"
      ],
      "metadata": {
        "id": "CPUeDPKIWcC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    CUSTOM KERAS LAYER: BASICS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"          CUSTOM KERAS LAYER: BASICS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class CustomDenseLayer(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Custom Dense layer demonstrating the Keras layer API.\n",
        "\n",
        "    Key methods:\n",
        "    - __init__: Store configuration (no weights yet!)\n",
        "    - build: Create weights when input shape is known\n",
        "    - call: Forward pass\n",
        "    - get_config: For serialization\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, units, activation=None, use_bias=True, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.activation = keras.activations.get(activation)\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        \"\"\"\n",
        "        Create weights. Called automatically the first time the layer is used.\n",
        "\n",
        "        self.add_weight() creates a tf.Variable and registers it properly.\n",
        "        \"\"\"\n",
        "        self.kernel = self.add_weight(\n",
        "            name='kernel',\n",
        "            shape=(input_shape[-1], self.units),\n",
        "            initializer='glorot_uniform',  # Xavier initialization\n",
        "            trainable=True\n",
        "        )\n",
        "\n",
        "        if self.use_bias:\n",
        "            self.bias = self.add_weight(\n",
        "                name='bias',\n",
        "                shape=(self.units,),\n",
        "                initializer='zeros',\n",
        "                trainable=True\n",
        "            )\n",
        "\n",
        "        # Mark as built\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"Forward pass.\"\"\"\n",
        "        output = tf.matmul(inputs, self.kernel)\n",
        "        if self.use_bias:\n",
        "            output = output + self.bias\n",
        "        if self.activation is not None:\n",
        "            output = self.activation(output)\n",
        "        return output\n",
        "\n",
        "    def get_config(self):\n",
        "        \"\"\"Enable serialization.\"\"\"\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            'units': self.units,\n",
        "            'activation': keras.activations.serialize(self.activation),\n",
        "            'use_bias': self.use_bias\n",
        "        })\n",
        "        return config\n",
        "\n",
        "# Test custom layer\n",
        "custom_dense = CustomDenseLayer(32, activation='relu')\n",
        "x = tf.random.normal((4, 16))\n",
        "y = custom_dense(x)  # This triggers build()\n",
        "\n",
        "print(f\"\\nCustomDenseLayer(32, activation='relu')\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"Kernel shape: {custom_dense.kernel.shape}\")\n",
        "print(f\"Trainable variables: {len(custom_dense.trainable_variables)}\")\n",
        "print(f\"\\nConfig: {custom_dense.get_config()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnKJDd05WcC_",
        "outputId": "9c73fdad-7a00-4168-c6fc-c5589fe170ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "          CUSTOM KERAS LAYER: BASICS\n",
            "============================================================\n",
            "\n",
            "CustomDenseLayer(32, activation='relu')\n",
            "Input shape:  (4, 16)\n",
            "Output shape: (4, 32)\n",
            "Kernel shape: (16, 32)\n",
            "Trainable variables: 2\n",
            "\n",
            "Config: {'name': 'custom_dense_layer', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}, 'units': 32, 'activation': 'relu', 'use_bias': True}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    CUSTOM LAYER: SELF-ATTENTION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"         CUSTOM LAYER: SELF-ATTENTION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class SelfAttentionLayer(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Self-Attention layer (simplified version of Transformer attention).\n",
        "\n",
        "    Attention(Q, K, V) = softmax(Q @ K^T / sqrt(d_k)) @ V\n",
        "\n",
        "    In self-attention, Q, K, V all come from the same input.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_dim, num_heads=1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Linear projections for Q, K, V\n",
        "        self.W_q = self.add_weight(\n",
        "            name='W_q',\n",
        "            shape=(input_shape[-1], self.embed_dim),\n",
        "            initializer='glorot_uniform'\n",
        "        )\n",
        "        self.W_k = self.add_weight(\n",
        "            name='W_k',\n",
        "            shape=(input_shape[-1], self.embed_dim),\n",
        "            initializer='glorot_uniform'\n",
        "        )\n",
        "        self.W_v = self.add_weight(\n",
        "            name='W_v',\n",
        "            shape=(input_shape[-1], self.embed_dim),\n",
        "            initializer='glorot_uniform'\n",
        "        )\n",
        "        self.W_o = self.add_weight(\n",
        "            name='W_o',\n",
        "            shape=(self.embed_dim, self.embed_dim),\n",
        "            initializer='glorot_uniform'\n",
        "        )\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        seq_len = tf.shape(inputs)[1]\n",
        "\n",
        "        # Linear projections\n",
        "        Q = inputs @ self.W_q  # (batch, seq, embed)\n",
        "        K = inputs @ self.W_k\n",
        "        V = inputs @ self.W_v\n",
        "\n",
        "        # Reshape for multi-head attention\n",
        "        Q = tf.reshape(Q, (batch_size, seq_len, self.num_heads, self.head_dim))\n",
        "        K = tf.reshape(K, (batch_size, seq_len, self.num_heads, self.head_dim))\n",
        "        V = tf.reshape(V, (batch_size, seq_len, self.num_heads, self.head_dim))\n",
        "\n",
        "        # Transpose to (batch, heads, seq, head_dim)\n",
        "        Q = tf.transpose(Q, [0, 2, 1, 3])\n",
        "        K = tf.transpose(K, [0, 2, 1, 3])\n",
        "        V = tf.transpose(V, [0, 2, 1, 3])\n",
        "\n",
        "        # Attention scores: Q @ K^T / sqrt(d_k)\n",
        "        scale = tf.sqrt(tf.cast(self.head_dim, tf.float32))\n",
        "        scores = tf.matmul(Q, K, transpose_b=True) / scale  # (batch, heads, seq, seq)\n",
        "\n",
        "        # Apply mask if provided\n",
        "        if mask is not None:\n",
        "            scores += (1 - mask) * -1e9\n",
        "\n",
        "        # Softmax\n",
        "        attention_weights = tf.nn.softmax(scores, axis=-1)\n",
        "\n",
        "        # Apply attention to values\n",
        "        context = tf.matmul(attention_weights, V)  # (batch, heads, seq, head_dim)\n",
        "\n",
        "        # Reshape back\n",
        "        context = tf.transpose(context, [0, 2, 1, 3])  # (batch, seq, heads, head_dim)\n",
        "        context = tf.reshape(context, (batch_size, seq_len, self.embed_dim))\n",
        "\n",
        "        # Output projection\n",
        "        output = context @ self.W_o\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            'embed_dim': self.embed_dim,\n",
        "            'num_heads': self.num_heads\n",
        "        })\n",
        "        return config\n",
        "\n",
        "# Test\n",
        "attention = SelfAttentionLayer(embed_dim=64, num_heads=4)\n",
        "x = tf.random.normal((2, 10, 64))  # (batch, seq_len, embed_dim)\n",
        "output, weights = attention(x)\n",
        "\n",
        "print(f\"\\nSelfAttentionLayer(embed_dim=64, num_heads=4)\")\n",
        "print(f\"Input shape:            {x.shape}\")\n",
        "print(f\"Output shape:           {output.shape}\")\n",
        "print(f\"Attention weights shape: {weights.shape}\")\n",
        "print(f\"Trainable parameters:   {sum(np.prod(v.shape) for v in attention.trainable_variables):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfKOP_WWWcDA",
        "outputId": "81cc04d4-21ee-4891-874c-68df91223e94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "         CUSTOM LAYER: SELF-ATTENTION\n",
            "============================================================\n",
            "\n",
            "SelfAttentionLayer(embed_dim=64, num_heads=4)\n",
            "Input shape:            (2, 10, 64)\n",
            "Output shape:           (2, 10, 64)\n",
            "Attention weights shape: (2, 4, 10, 10)\n",
            "Trainable parameters:   16,384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    CUSTOM LAYER: SPECTRAL NORMALIZATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"       CUSTOM LAYER: SPECTRAL NORMALIZATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class SpectralNormalization(keras.layers.Wrapper):\n",
        "    \"\"\"\n",
        "    Spectral Normalization wrapper for layers.\n",
        "\n",
        "    Constrains the spectral norm (largest singular value) of the weight matrix.\n",
        "    Used in GANs to stabilize training.\n",
        "\n",
        "    W_normalized = W / sigma(W)\n",
        "    where sigma(W) is the largest singular value.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, layer, power_iterations=1, epsilon=1e-12, **kwargs):\n",
        "        super().__init__(layer, **kwargs)\n",
        "        self.power_iterations = power_iterations\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.layer.build(input_shape)\n",
        "\n",
        "        # Get the weight matrix\n",
        "        self.w = self.layer.kernel\n",
        "        w_shape = self.w.shape.as_list()\n",
        "\n",
        "        # Flatten weight to 2D for SVD\n",
        "        self.w_flat_shape = (np.prod(w_shape[:-1]), w_shape[-1])\n",
        "\n",
        "        # Initialize u vector (for power iteration)\n",
        "        self.u = self.add_weight(\n",
        "            name='u',\n",
        "            shape=(1, w_shape[-1]),\n",
        "            initializer='random_normal',\n",
        "            trainable=False\n",
        "        )\n",
        "\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, training=True):\n",
        "        # Power iteration to estimate largest singular value\n",
        "        w_flat = tf.reshape(self.w, self.w_flat_shape)\n",
        "\n",
        "        u = self.u\n",
        "        for _ in range(self.power_iterations):\n",
        "            # v = W^T u / ||W^T u||\n",
        "            v = tf.matmul(u, tf.transpose(w_flat))\n",
        "            v = v / (tf.norm(v, ord='euclidean') + self.epsilon)\n",
        "\n",
        "            # u = W v / ||W v||\n",
        "            u = tf.matmul(v, w_flat)\n",
        "            u = u / (tf.norm(u, ord='euclidean') + self.epsilon)\n",
        "\n",
        "        if training:\n",
        "            self.u.assign(u)\n",
        "\n",
        "        # Spectral norm: sigma = u^T W v\n",
        "        # Corrected calculation for row vectors u (1, out_features) and v (1, in_features)\n",
        "        # The formula should be v W u^T\n",
        "        sigma = tf.matmul(tf.matmul(v, w_flat), tf.transpose(u))\n",
        "\n",
        "        # Normalize weight\n",
        "        w_normalized = self.w / sigma[0, 0]\n",
        "\n",
        "        # Manually perform the forward pass of the wrapped layer using w_normalized.\n",
        "        # This assumes the wrapped layer is a Dense layer based on the test case.\n",
        "        output = tf.matmul(inputs, w_normalized)\n",
        "\n",
        "        # If the wrapped layer has a bias and uses it, add it.\n",
        "        if hasattr(self.layer, 'bias') and self.layer.use_bias:\n",
        "            output = output + self.layer.bias\n",
        "\n",
        "        # If the wrapped layer has an activation function, apply it.\n",
        "        if hasattr(self.layer, 'activation') and self.layer.activation is not None:\n",
        "            output = self.layer.activation(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "# Test\n",
        "base_layer = layers.Dense(64)\n",
        "spectral_dense = SpectralNormalization(base_layer)\n",
        "x = tf.random.normal((4, 32))\n",
        "y = spectral_dense(x)\n",
        "\n",
        "print(f\"\\nSpectralNormalization(Dense(64))\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"\\n Use case: Stabilize GAN discriminator training\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFYq5xJbWcDA",
        "outputId": "485a6e98-1972-45d9-cd20-d3fbd5f5d73e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "       CUSTOM LAYER: SPECTRAL NORMALIZATION\n",
            "============================================================\n",
            "\n",
            "SpectralNormalization(Dense(64))\n",
            "Input shape:  (4, 32)\n",
            "Output shape: (4, 64)\n",
            "\n",
            " Use case: Stabilize GAN discriminator training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Part V: Advanced Architectures\n",
        "\n",
        "## Building Modern Deep Learning Components\n",
        "\n",
        "Now let's build advanced architectural components used in state-of-the-art models:\n",
        "\n",
        "- **Residual Blocks** (ResNet)\n",
        "- **Squeeze-and-Excitation** (SENet)\n",
        "- **Transformer Encoder Block**\n",
        "- **Custom Normalization Layers**"
      ],
      "metadata": {
        "id": "biOHWrOFWcDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    RESIDUAL BLOCK (ResNet Style)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"           RESIDUAL BLOCK (ResNet)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class ResidualBlock(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Residual Block: y = F(x) + x\n",
        "\n",
        "    The key insight: learning residual F(x) = y - x is easier than learning y directly.\n",
        "    This enables training very deep networks (100+ layers).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, filters, kernel_size=3, stride=1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.filters = filters\n",
        "        self.stride = stride\n",
        "\n",
        "        # Main path\n",
        "        self.conv1 = layers.Conv2D(filters, kernel_size, strides=stride,\n",
        "                                    padding='same', use_bias=False)\n",
        "        self.bn1 = layers.BatchNormalization()\n",
        "\n",
        "        self.conv2 = layers.Conv2D(filters, kernel_size, strides=1,\n",
        "                                    padding='same', use_bias=False)\n",
        "        self.bn2 = layers.BatchNormalization()\n",
        "\n",
        "        # Skip connection (identity or projection)\n",
        "        self.skip_conv = None\n",
        "        self.skip_bn = None\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Need projection if dimensions change\n",
        "        if input_shape[-1] != self.filters or self.stride != 1:\n",
        "            self.skip_conv = layers.Conv2D(self.filters, 1, strides=self.stride,\n",
        "                                           padding='same', use_bias=False)\n",
        "            self.skip_bn = layers.BatchNormalization()\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        # Main path\n",
        "        x = self.conv1(inputs)\n",
        "        x = self.bn1(x, training=training)\n",
        "        x = tf.nn.relu(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x, training=training)\n",
        "\n",
        "        # Skip connection\n",
        "        if self.skip_conv is not None:\n",
        "            skip = self.skip_conv(inputs)\n",
        "            skip = self.skip_bn(skip, training=training)\n",
        "        else:\n",
        "            skip = inputs\n",
        "\n",
        "        # Add and activate\n",
        "        return tf.nn.relu(x + skip)\n",
        "\n",
        "# Test\n",
        "res_block = ResidualBlock(64, stride=1)\n",
        "x = tf.random.normal((2, 32, 32, 64))\n",
        "y = res_block(x)\n",
        "\n",
        "print(f\"\\nResidualBlock(64)\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "\n",
        "# With downsampling\n",
        "res_block_down = ResidualBlock(128, stride=2)\n",
        "y_down = res_block_down(x)\n",
        "print(f\"\\nResidualBlock(128, stride=2)\")\n",
        "print(f\"Output shape: {y_down.shape}  (spatial dims halved, channels doubled)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqnNTvJTWcDA",
        "outputId": "d27be267-7a99-4e56-a75a-bcb8a2ff6c03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "           RESIDUAL BLOCK (ResNet)\n",
            "============================================================\n",
            "\n",
            "ResidualBlock(64)\n",
            "Input shape:  (2, 32, 32, 64)\n",
            "Output shape: (2, 32, 32, 64)\n",
            "\n",
            "ResidualBlock(128, stride=2)\n",
            "Output shape: (2, 16, 16, 128)  (spatial dims halved, channels doubled)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    SQUEEZE-AND-EXCITATION BLOCK\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"         SQUEEZE-AND-EXCITATION BLOCK\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class SqueezeExcitationBlock(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Squeeze-and-Excitation (SE) Block.\n",
        "\n",
        "    Learns channel-wise attention weights:\n",
        "    1. Squeeze: Global average pooling (H,W,C) -> (1,1,C)\n",
        "    2. Excitation: FC -> ReLU -> FC -> Sigmoid\n",
        "    3. Scale: Multiply input by attention weights\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, reduction_ratio=16, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.reduction_ratio = reduction_ratio\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        channels = input_shape[-1]\n",
        "        reduced_channels = max(channels // self.reduction_ratio, 1)\n",
        "\n",
        "        self.fc1 = layers.Dense(reduced_channels, activation='relu')\n",
        "        self.fc2 = layers.Dense(channels, activation='sigmoid')\n",
        "\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Squeeze: Global Average Pooling\n",
        "        squeezed = tf.reduce_mean(inputs, axis=[1, 2], keepdims=True)  # (B, 1, 1, C)\n",
        "\n",
        "        # Excitation\n",
        "        x = tf.reshape(squeezed, (tf.shape(inputs)[0], -1))  # (B, C)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = tf.reshape(x, (tf.shape(inputs)[0], 1, 1, -1))  # (B, 1, 1, C)\n",
        "\n",
        "        # Scale\n",
        "        return inputs * x\n",
        "\n",
        "# Test\n",
        "se_block = SqueezeExcitationBlock(reduction_ratio=16)\n",
        "x = tf.random.normal((2, 28, 28, 64))\n",
        "y = se_block(x)\n",
        "\n",
        "print(f\"\\nSqueezeExcitationBlock(reduction_ratio=16)\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"\\n SE learns which channels are important for the task\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcm-_VhvWcDA",
        "outputId": "8c929ee3-8964-465a-f7f0-05baab0ed043"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "         SQUEEZE-AND-EXCITATION BLOCK\n",
            "============================================================\n",
            "\n",
            "SqueezeExcitationBlock(reduction_ratio=16)\n",
            "Input shape:  (2, 28, 28, 64)\n",
            "Output shape: (2, 28, 28, 64)\n",
            "\n",
            " SE learns which channels are important for the task\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    TRANSFORMER ENCODER BLOCK\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"          TRANSFORMER ENCODER BLOCK\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class TransformerEncoderBlock(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Transformer Encoder Block.\n",
        "\n",
        "    Architecture:\n",
        "        x -> LayerNorm -> MultiHeadAttention -> + (residual) ->\n",
        "             LayerNorm -> FeedForward -> + (residual) -> output\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.ff_dim = ff_dim\n",
        "\n",
        "        # Multi-head attention\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=embed_dim // num_heads\n",
        "        )\n",
        "\n",
        "        # Feed-forward network\n",
        "        self.ffn = Sequential([\n",
        "            layers.Dense(ff_dim, activation='gelu'),\n",
        "            layers.Dropout(dropout_rate),\n",
        "            layers.Dense(embed_dim),\n",
        "            layers.Dropout(dropout_rate)\n",
        "        ])\n",
        "\n",
        "        # Layer normalization\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout1 = layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, inputs, training=False, mask=None):\n",
        "        # Pre-norm architecture (more stable)\n",
        "        # Attention block\n",
        "        x = self.layernorm1(inputs)\n",
        "        attn_output = self.attention(x, x, attention_mask=mask, training=training)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        x = inputs + attn_output  # Residual connection\n",
        "\n",
        "        # Feed-forward block\n",
        "        ffn_input = self.layernorm2(x)\n",
        "        ffn_output = self.ffn(ffn_input, training=training)\n",
        "\n",
        "        return x + ffn_output  # Residual connection\n",
        "\n",
        "# Test\n",
        "transformer_block = TransformerEncoderBlock(\n",
        "    embed_dim=64,\n",
        "    num_heads=4,\n",
        "    ff_dim=256,\n",
        "    dropout_rate=0.1\n",
        ")\n",
        "\n",
        "x = tf.random.normal((2, 20, 64))  # (batch, seq_len, embed_dim)\n",
        "y = transformer_block(x)\n",
        "\n",
        "print(f\"\\nTransformerEncoderBlock(embed_dim=64, num_heads=4, ff_dim=256)\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"Parameters:   {sum(np.prod(v.shape) for v in transformer_block.trainable_variables):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1I-38YHnWcDA",
        "outputId": "e1769ebc-3550-4b1f-f6fd-aaa9196218c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "          TRANSFORMER ENCODER BLOCK\n",
            "============================================================\n",
            "\n",
            "TransformerEncoderBlock(embed_dim=64, num_heads=4, ff_dim=256)\n",
            "Input shape:  (2, 20, 64)\n",
            "Output shape: (2, 20, 64)\n",
            "Parameters:   49,984\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Part VI: Custom Training Loops\n",
        "\n",
        "## Full Control with GradientTape\n",
        "\n",
        "While `model.fit()` is convenient, custom training loops give you:\n",
        "\n",
        "- **Complete control** over the training process\n",
        "- **Custom metrics** and logging\n",
        "- **Complex training schemes** (GANs, reinforcement learning)\n",
        "- **Gradient manipulation** (clipping, accumulation)\n",
        "- **Multi-GPU/TPU** strategies"
      ],
      "metadata": {
        "id": "z13eQIQpWcDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    BASIC CUSTOM TRAINING LOOP\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"          BASIC CUSTOM TRAINING LOOP\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def custom_training_loop(model, train_data, val_data, epochs, learning_rate=0.001):\n",
        "    \"\"\"\n",
        "    Complete custom training loop with GradientTape.\n",
        "    \"\"\"\n",
        "    optimizer = keras.optimizers.Adam(learning_rate)\n",
        "    loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "    # Metrics\n",
        "    train_loss = keras.metrics.Mean()\n",
        "    train_acc = keras.metrics.SparseCategoricalAccuracy()\n",
        "    val_loss = keras.metrics.Mean()\n",
        "    val_acc = keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Reset metrics\n",
        "        train_loss.reset_state()\n",
        "        train_acc.reset_state()\n",
        "\n",
        "        # Training loop\n",
        "        for x_batch, y_batch in train_data:\n",
        "            with tf.GradientTape() as tape:\n",
        "                # Forward pass\n",
        "                predictions = model(x_batch, training=True)\n",
        "                loss = loss_fn(y_batch, predictions)\n",
        "\n",
        "            # Backward pass\n",
        "            gradients = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "            # Update weights\n",
        "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "            # Update metrics\n",
        "            train_loss.update_state(loss)\n",
        "            train_acc.update_state(y_batch, predictions)\n",
        "\n",
        "        # Validation loop\n",
        "        val_loss.reset_state()\n",
        "        val_acc.reset_state()\n",
        "\n",
        "        for x_batch, y_batch in val_data:\n",
        "            predictions = model(x_batch, training=False)\n",
        "            loss = loss_fn(y_batch, predictions)\n",
        "            val_loss.update_state(loss)\n",
        "            val_acc.update_state(y_batch, predictions)\n",
        "\n",
        "        # Record history\n",
        "        history['train_loss'].append(train_loss.result().numpy())\n",
        "        history['train_acc'].append(train_acc.result().numpy())\n",
        "        history['val_loss'].append(val_loss.result().numpy())\n",
        "        history['val_acc'].append(val_acc.result().numpy())\n",
        "\n",
        "        # Print progress\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | \"\n",
        "              f\"Train Loss: {train_loss.result():.4f} | \"\n",
        "              f\"Train Acc: {train_acc.result():.4f} | \"\n",
        "              f\"Val Loss: {val_loss.result():.4f} | \"\n",
        "              f\"Val Acc: {val_acc.result():.4f}\")\n",
        "\n",
        "    return history\n",
        "\n",
        "print(\"\\nCustom training loop template created!\")\n",
        "print(\"Key components:\")\n",
        "print(\"  1. GradientTape for computing gradients\")\n",
        "print(\"  2. optimizer.apply_gradients() for weight updates\")\n",
        "print(\"  3. Metrics for tracking performance\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0S7sZ5woWcDA",
        "outputId": "03d6abae-b680-4c46-9340-de1a79f51597"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "          BASIC CUSTOM TRAINING LOOP\n",
            "============================================================\n",
            "\n",
            "Custom training loop template created!\n",
            "Key components:\n",
            "  1. GradientTape for computing gradients\n",
            "  2. optimizer.apply_gradients() for weight updates\n",
            "  3. Metrics for tracking performance\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    TRAINING WITH GRADIENT CLIPPING\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"        TRAINING WITH GRADIENT CLIPPING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "@tf.function\n",
        "def train_step_with_clipping(model, x, y, optimizer, loss_fn, clip_norm=1.0):\n",
        "    \"\"\"\n",
        "    Single training step with gradient clipping.\n",
        "\n",
        "    Gradient clipping prevents exploding gradients in deep networks.\n",
        "    \"\"\"\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(x, training=True)\n",
        "        loss = loss_fn(y, predictions)\n",
        "\n",
        "    # Compute gradients\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "    # Clip gradients by global norm\n",
        "    gradients, global_norm = tf.clip_by_global_norm(gradients, clip_norm)\n",
        "\n",
        "    # Apply clipped gradients\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    return loss, global_norm\n",
        "\n",
        "print(\"Gradient Clipping Options:\")\n",
        "print(\"  - tf.clip_by_value(g, -clip, clip)  : Clip element-wise\")\n",
        "print(\"  - tf.clip_by_norm(g, clip)          : Clip each tensor by L2 norm\")\n",
        "print(\"  - tf.clip_by_global_norm(grads, clip): Clip all gradients together\")\n",
        "print(\"\\nGlobal norm is most common - maintains gradient direction!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nt4KslvnWcDA",
        "outputId": "1ad2bc98-d25a-44f9-9769-6d86d44cb354"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "        TRAINING WITH GRADIENT CLIPPING\n",
            "============================================================\n",
            "Gradient Clipping Options:\n",
            "  - tf.clip_by_value(g, -clip, clip)  : Clip element-wise\n",
            "  - tf.clip_by_norm(g, clip)          : Clip each tensor by L2 norm\n",
            "  - tf.clip_by_global_norm(grads, clip): Clip all gradients together\n",
            "\n",
            "Global norm is most common - maintains gradient direction!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    CUSTOM MODEL WITH train_step()\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"        CUSTOM MODEL WITH train_step()\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class CustomTrainableModel(keras.Model):\n",
        "    \"\"\"\n",
        "    Model with custom training logic built-in.\n",
        "\n",
        "    Override train_step() to customize what happens in model.fit().\n",
        "    This is the best of both worlds:\n",
        "    - Custom training logic\n",
        "    - Still use model.fit() with callbacks, validation, etc.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, units_list, num_classes, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.dense_layers = []\n",
        "        for units in units_list:\n",
        "            self.dense_layers.append(layers.Dense(units, activation='relu'))\n",
        "            self.dense_layers.append(layers.Dropout(0.2))\n",
        "\n",
        "        self.output_layer = layers.Dense(num_classes, activation='softmax')\n",
        "\n",
        "        # Custom metrics\n",
        "        self.loss_tracker = keras.metrics.Mean(name='loss')\n",
        "        self.acc_tracker = keras.metrics.SparseCategoricalAccuracy(name='accuracy')\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = inputs\n",
        "        for layer in self.dense_layers:\n",
        "            x = layer(x, training=training)\n",
        "        return self.output_layer(x)\n",
        "\n",
        "    def train_step(self, data):\n",
        "        \"\"\"\n",
        "        Custom training step.\n",
        "        Called by model.fit() for each batch.\n",
        "        \"\"\"\n",
        "        x, y = data\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = self(x, training=True)\n",
        "            loss = keras.losses.sparse_categorical_crossentropy(y, y_pred)\n",
        "            loss = tf.reduce_mean(loss)\n",
        "\n",
        "        # Compute and apply gradients\n",
        "        gradients = tape.gradient(loss, self.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "\n",
        "        # Update metrics\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        self.acc_tracker.update_state(y, y_pred)\n",
        "\n",
        "        return {\n",
        "            'loss': self.loss_tracker.result(),\n",
        "            'accuracy': self.acc_tracker.result()\n",
        "        }\n",
        "\n",
        "    def test_step(self, data):\n",
        "        \"\"\"Custom evaluation step.\"\"\"\n",
        "        x, y = data\n",
        "        y_pred = self(x, training=False)\n",
        "        loss = keras.losses.sparse_categorical_crossentropy(y, y_pred)\n",
        "\n",
        "        self.loss_tracker.update_state(tf.reduce_mean(loss))\n",
        "        self.acc_tracker.update_state(y, y_pred)\n",
        "\n",
        "        return {\n",
        "            'loss': self.loss_tracker.result(),\n",
        "            'accuracy': self.acc_tracker.result()\n",
        "        }\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.loss_tracker, self.acc_tracker]\n",
        "\n",
        "# Test\n",
        "custom_model = CustomTrainableModel([64, 32], num_classes=10)\n",
        "custom_model.compile(optimizer='adam')\n",
        "\n",
        "print(\"\\nCustom model with train_step() created!\")\n",
        "print(\"Now model.fit() uses our custom training logic\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ocCaxVeWcDA",
        "outputId": "c427abe3-cb43-4aa5-9794-4ec3899116e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "        CUSTOM MODEL WITH train_step()\n",
            "============================================================\n",
            "\n",
            "Custom model with train_step() created!\n",
            "Now model.fit() uses our custom training logic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Part VII: Practical Demos\n",
        "\n",
        "## Putting It All Together\n",
        "\n",
        "Let's combine everything we've learned in real examples."
      ],
      "metadata": {
        "id": "S6cCw5HyWcDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#              DEMO 1: CUSTOM RESNET FOR DIGIT CLASSIFICATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"     DEMO: CUSTOM RESNET FOR DIGIT CLASSIFICATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load digits dataset\n",
        "digits = load_digits()\n",
        "X, y = digits.data, digits.target\n",
        "\n",
        "# Preprocess\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Reshape for CNN: (samples, 8, 8, 1)\n",
        "X = X.reshape(-1, 8, 8, 1).astype(np.float32)\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training samples: {X_train.shape[0]}\")\n",
        "print(f\"Test samples:     {X_test.shape[0]}\")\n",
        "print(f\"Image shape:      {X_train.shape[1:]}\")\n",
        "\n",
        "# Build custom ResNet model\n",
        "class MiniResNet(keras.Model):\n",
        "    \"\"\"Mini ResNet with custom residual blocks.\"\"\"\n",
        "\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "\n",
        "        # Initial convolution\n",
        "        self.conv1 = layers.Conv2D(32, 3, padding='same', activation='relu')\n",
        "\n",
        "        # Residual blocks\n",
        "        self.res_block1 = ResidualBlock(32)\n",
        "        self.res_block2 = ResidualBlock(64, stride=2)\n",
        "\n",
        "        # SE block for channel attention\n",
        "        self.se_block = SqueezeExcitationBlock(reduction_ratio=8)\n",
        "\n",
        "        # Classification head\n",
        "        self.global_pool = layers.GlobalAveragePooling2D()\n",
        "        self.dropout = layers.Dropout(0.3)\n",
        "        self.dense = layers.Dense(num_classes, activation='softmax')\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        x = self.conv1(x)\n",
        "        x = self.res_block1(x, training=training)\n",
        "        x = self.res_block2(x, training=training)\n",
        "        x = self.se_block(x)\n",
        "        x = self.global_pool(x)\n",
        "        x = self.dropout(x, training=training)\n",
        "        return self.dense(x)\n",
        "\n",
        "# Create and compile\n",
        "tf.random.set_seed(42)\n",
        "resnet_model = MiniResNet(num_classes=10)\n",
        "\n",
        "resnet_model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Build model by calling it\n",
        "_ = resnet_model(X_train[:1])\n",
        "resnet_model.summary()\n",
        "\n",
        "# Train\n",
        "print(\"\\nTraining MiniResNet...\")\n",
        "history = resnet_model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=30,\n",
        "    batch_size=32,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "test_loss, test_acc = resnet_model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"\\nTest Accuracy: {test_acc*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hJng8MQqWcDA",
        "outputId": "efb35d43-b35f-4876-cdd4-dfe9e8a11648"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "     DEMO: CUSTOM RESNET FOR DIGIT CLASSIFICATION\n",
            "============================================================\n",
            "Training samples: 1437\n",
            "Test samples:     360\n",
            "Image shape:      (8, 8, 1)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"mini_res_net\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"mini_res_net\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)                (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)                     \u001b[38;5;34m320\u001b[0m \n",
              "\n",
              " residual_block_2                 ?                              \u001b[38;5;34m18,688\u001b[0m \n",
              " (\u001b[38;5;33mResidualBlock\u001b[0m)                                                        \n",
              "\n",
              " residual_block_3                 ?                              \u001b[38;5;34m58,112\u001b[0m \n",
              " (\u001b[38;5;33mResidualBlock\u001b[0m)                                                        \n",
              "\n",
              " squeeze_excitation_block_1       ?                               \u001b[38;5;34m1,096\u001b[0m \n",
              " (\u001b[38;5;33mSqueezeExcitationBlock\u001b[0m)                                               \n",
              "\n",
              " global_average_pooling2d         ?                                   \u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                               \n",
              "\n",
              " dropout_6 (\u001b[38;5;33mDropout\u001b[0m)              ?                                   \u001b[38;5;34m0\u001b[0m \n",
              "\n",
              " dense_11 (\u001b[38;5;33mDense\u001b[0m)                 (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m10\u001b[0m)                           \u001b[38;5;34m650\u001b[0m \n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)                    </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">       Param # </span>\n",
              "\n",
              " conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> \n",
              "\n",
              " residual_block_2                 ?                              <span style=\"color: #00af00; text-decoration-color: #00af00\">18,688</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ResidualBlock</span>)                                                        \n",
              "\n",
              " residual_block_3                 ?                              <span style=\"color: #00af00; text-decoration-color: #00af00\">58,112</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ResidualBlock</span>)                                                        \n",
              "\n",
              " squeeze_excitation_block_1       ?                               <span style=\"color: #00af00; text-decoration-color: #00af00\">1,096</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SqueezeExcitationBlock</span>)                                               \n",
              "\n",
              " global_average_pooling2d         ?                                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                               \n",
              "\n",
              " dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              ?                                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "\n",
              " dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> \n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m78,866\u001b[0m (308.07 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">78,866</span> (308.07 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m78,354\u001b[0m (306.07 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">78,354</span> (306.07 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m512\u001b[0m (2.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> (2.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training MiniResNet...\n",
            "Epoch 1/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 30ms/step - accuracy: 0.4196 - loss: 1.9130 - val_accuracy: 0.4222 - val_loss: 2.1686\n",
            "Epoch 2/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.8976 - loss: 0.7415 - val_accuracy: 0.0889 - val_loss: 2.1293\n",
            "Epoch 3/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9572 - loss: 0.2860 - val_accuracy: 0.0833 - val_loss: 2.8944\n",
            "Epoch 4/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9810 - loss: 0.1257 - val_accuracy: 0.0833 - val_loss: 3.8577\n",
            "Epoch 5/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9930 - loss: 0.0701 - val_accuracy: 0.2028 - val_loss: 2.5385\n",
            "Epoch 6/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9960 - loss: 0.0496 - val_accuracy: 0.4194 - val_loss: 1.4897\n",
            "Epoch 7/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9964 - loss: 0.0334 - val_accuracy: 0.6167 - val_loss: 0.9665\n",
            "Epoch 8/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0237 - val_accuracy: 0.8694 - val_loss: 0.3691\n",
            "Epoch 9/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.9982 - loss: 0.0220 - val_accuracy: 0.9500 - val_loss: 0.1440\n",
            "Epoch 10/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.9997 - loss: 0.0127 - val_accuracy: 0.9667 - val_loss: 0.1076\n",
            "Epoch 11/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9999 - loss: 0.0165 - val_accuracy: 0.9389 - val_loss: 0.1869\n",
            "Epoch 12/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9989 - loss: 0.0139 - val_accuracy: 0.9611 - val_loss: 0.0910\n",
            "Epoch 13/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0116 - val_accuracy: 0.9722 - val_loss: 0.0751\n",
            "Epoch 14/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0073 - val_accuracy: 0.9972 - val_loss: 0.0214\n",
            "Epoch 15/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0072 - val_accuracy: 0.9861 - val_loss: 0.0336\n",
            "Epoch 16/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9991 - loss: 0.0080 - val_accuracy: 0.9389 - val_loss: 0.1819\n",
            "Epoch 17/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9972 - loss: 0.0155 - val_accuracy: 0.8333 - val_loss: 0.8679\n",
            "Epoch 18/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9943 - loss: 0.0351 - val_accuracy: 0.9556 - val_loss: 0.1431\n",
            "Epoch 19/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9984 - loss: 0.0234 - val_accuracy: 0.9861 - val_loss: 0.0354\n",
            "Epoch 20/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0079 - val_accuracy: 0.9889 - val_loss: 0.0265\n",
            "Epoch 21/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.9947 - loss: 0.0108 - val_accuracy: 0.9889 - val_loss: 0.0313\n",
            "Epoch 22/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0047 - val_accuracy: 0.9944 - val_loss: 0.0176\n",
            "Epoch 23/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0030 - val_accuracy: 0.9944 - val_loss: 0.0230\n",
            "Epoch 24/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0022 - val_accuracy: 0.9972 - val_loss: 0.0171\n",
            "Epoch 25/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0017 - val_accuracy: 0.9972 - val_loss: 0.0171\n",
            "Epoch 26/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 0.9944 - val_loss: 0.0145\n",
            "Epoch 27/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0019 - val_accuracy: 0.9944 - val_loss: 0.0126\n",
            "Epoch 28/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0017 - val_accuracy: 1.0000 - val_loss: 0.0149\n",
            "Epoch 29/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0017 - val_accuracy: 1.0000 - val_loss: 0.0106\n",
            "Epoch 30/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0022 - val_accuracy: 0.9972 - val_loss: 0.0145\n",
            "\n",
            "Test Accuracy: 99.72%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    VISUALIZE TRAINING RESULTS\n",
        "# ============================================================================\n",
        "\n",
        "# Plot training history\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "ax1.plot(history.history['loss'], 'b-', label='Train Loss')\n",
        "ax1.plot(history.history['val_loss'], 'r-', label='Val Loss')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.set_title('MiniResNet Training Loss', fontweight='bold')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "ax2.plot(history.history['accuracy'], 'b-', label='Train Acc')\n",
        "ax2.plot(history.history['val_accuracy'], 'r-', label='Val Acc')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.set_title('MiniResNet Training Accuracy', fontweight='bold')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Visualize predictions\n",
        "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
        "fig.suptitle('MiniResNet Predictions', fontsize=14, fontweight='bold')\n",
        "\n",
        "predictions = resnet_model.predict(X_test[:10], verbose=0)\n",
        "pred_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "for i, ax in enumerate(axes.flatten()):\n",
        "    img = X_test[i].reshape(8, 8)\n",
        "    true_label = y_test[i]\n",
        "    pred_label = pred_classes[i]\n",
        "    confidence = predictions[i][pred_label] * 100\n",
        "\n",
        "    ax.imshow(img, cmap='gray')\n",
        "    color = 'green' if true_label == pred_label else 'red'\n",
        "    ax.set_title(f'True: {true_label}, Pred: {pred_label}\\n({confidence:.1f}%)',\n",
        "                 color=color, fontsize=10)\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "6QkXz-l8WcDA",
        "outputId": "63b38de3-320b-4e9b-be74-168dd7ca186b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1400x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x600 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                         ADVANCED CHEAT SHEET\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"              ADVANCED TENSORFLOW/KERAS CHEAT SHEET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "cheat_sheet = \"\"\"\n",
        "GRADIENTTAPE PATTERNS\n",
        "---------------------\n",
        "# Basic gradient\n",
        "with tf.GradientTape() as tape:\n",
        "    y = model(x)\n",
        "grads = tape.gradient(y, model.trainable_variables)\n",
        "\n",
        "# Higher-order derivatives (nested tapes)\n",
        "with tf.GradientTape() as t2:\n",
        "    with tf.GradientTape() as t1:\n",
        "        y = f(x)\n",
        "    dy = t1.gradient(y, x)\n",
        "d2y = t2.gradient(dy, x)\n",
        "\n",
        "# Jacobian\n",
        "jacobian = tape.jacobian(y, x)\n",
        "\n",
        "# Custom gradient\n",
        "@tf.custom_gradient\n",
        "def custom_op(x):\n",
        "    def grad(dy):\n",
        "        return dy * custom_backward\n",
        "    return forward_result, grad\n",
        "\n",
        "CUSTOM KERAS LAYERS\n",
        "-------------------\n",
        "class CustomLayer(keras.layers.Layer):\n",
        "    def __init__(self, units, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.units = units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.kernel = self.add_weight(\n",
        "            shape=(input_shape[-1], self.units),\n",
        "            initializer='glorot_uniform',\n",
        "            trainable=True\n",
        "        )\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        return inputs @ self.kernel\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config['units'] = self.units\n",
        "        return config\n",
        "\n",
        "CUSTOM TRAINING\n",
        "---------------\n",
        "# Override train_step for model.fit()\n",
        "class CustomModel(keras.Model):\n",
        "    def train_step(self, data):\n",
        "        x, y = data\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = self(x, training=True)\n",
        "            loss = self.compute_loss(y=y, y_pred=y_pred)\n",
        "        grads = tape.gradient(loss, self.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
        "        return {'loss': loss}\n",
        "\n",
        "GRADIENT MANIPULATION\n",
        "---------------------\n",
        "# Clip by global norm\n",
        "grads, _ = tf.clip_by_global_norm(grads, max_norm=1.0)\n",
        "\n",
        "# Gradient accumulation\n",
        "accumulated = [acc + g/steps for acc, g in zip(accumulated, grads)]\n",
        "\"\"\"\n",
        "print(cheat_sheet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5Z_iXlFWcDA",
        "outputId": "f6138fd7-40a3-4876-cbea-3a2fbd7426d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "              ADVANCED TENSORFLOW/KERAS CHEAT SHEET\n",
            "======================================================================\n",
            "\n",
            "GRADIENTTAPE PATTERNS\n",
            "---------------------\n",
            "# Basic gradient\n",
            "with tf.GradientTape() as tape:\n",
            "    y = model(x)\n",
            "grads = tape.gradient(y, model.trainable_variables)\n",
            "\n",
            "# Higher-order derivatives (nested tapes)\n",
            "with tf.GradientTape() as t2:\n",
            "    with tf.GradientTape() as t1:\n",
            "        y = f(x)\n",
            "    dy = t1.gradient(y, x)\n",
            "d2y = t2.gradient(dy, x)\n",
            "\n",
            "# Jacobian\n",
            "jacobian = tape.jacobian(y, x)\n",
            "\n",
            "# Custom gradient\n",
            "@tf.custom_gradient\n",
            "def custom_op(x):\n",
            "    def grad(dy):\n",
            "        return dy * custom_backward\n",
            "    return forward_result, grad\n",
            "\n",
            "CUSTOM KERAS LAYERS\n",
            "-------------------\n",
            "class CustomLayer(keras.layers.Layer):\n",
            "    def __init__(self, units, **kwargs):\n",
            "        super().__init__(**kwargs)\n",
            "        self.units = units\n",
            "    \n",
            "    def build(self, input_shape):\n",
            "        self.kernel = self.add_weight(\n",
            "            shape=(input_shape[-1], self.units),\n",
            "            initializer='glorot_uniform',\n",
            "            trainable=True\n",
            "        )\n",
            "        super().build(input_shape)\n",
            "    \n",
            "    def call(self, inputs, training=False):\n",
            "        return inputs @ self.kernel\n",
            "    \n",
            "    def get_config(self):\n",
            "        config = super().get_config()\n",
            "        config['units'] = self.units\n",
            "        return config\n",
            "\n",
            "CUSTOM TRAINING\n",
            "---------------\n",
            "# Override train_step for model.fit()\n",
            "class CustomModel(keras.Model):\n",
            "    def train_step(self, data):\n",
            "        x, y = data\n",
            "        with tf.GradientTape() as tape:\n",
            "            y_pred = self(x, training=True)\n",
            "            loss = self.compute_loss(y=y, y_pred=y_pred)\n",
            "        grads = tape.gradient(loss, self.trainable_variables)\n",
            "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
            "        return {'loss': loss}\n",
            "\n",
            "GRADIENT MANIPULATION\n",
            "---------------------\n",
            "# Clip by global norm\n",
            "grads, _ = tf.clip_by_global_norm(grads, max_norm=1.0)\n",
            "\n",
            "# Gradient accumulation\n",
            "accumulated = [acc + g/steps for acc, g in zip(accumulated, grads)]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Conclusion\n",
        "\n",
        "## Your Advanced TensorFlow & Keras Journey\n",
        "\n",
        "Congratulations! You've mastered advanced TensorFlow and Keras techniques.\n",
        "\n",
        "### What You Learned\n",
        "\n",
        "| Part | Topic | Key Takeaway |\n",
        "|------|-------|-------------|\n",
        "| I | Advanced GradientTape | Nested tapes, Jacobians, custom gradients |\n",
        "| II | Building Ops | Conv, pooling, normalization from scratch |\n",
        "| III | Primitive Layers | Dense, Conv2D with only tf.Variable |\n",
        "| IV | Custom Keras Layers | Proper subclassing with build() and call() |\n",
        "| V | Advanced Architectures | ResNet, SE-Net, Transformer blocks |\n",
        "| VI | Custom Training | Full control with GradientTape |\n",
        "| VII | Practical Demos | Real-world model combining everything |\n",
        "\n",
        "### When to Use What\n",
        "\n",
        "| Approach | Use When |\n",
        "|----------|----------|\n",
        "| `model.fit()` | Standard training, quick prototyping |\n",
        "| Custom `train_step()` | Custom logic but want callbacks/validation |\n",
        "| Full GradientTape loop | GANs, RL, complex multi-model training |\n",
        "| Custom layers | Reusable components, research |\n",
        "| Primitive layers | Learning, debugging, maximum control |\n",
        "\n",
        "### The Complete Learning Path\n",
        "\n",
        "1. **NumPy from Scratch** - Understand the math deeply\n",
        "2. **PyTorch** - Research-friendly framework\n",
        "3. **TensorFlow/Keras Part 1** - Fundamentals and high-level API\n",
        "4. **TensorFlow/Keras Part 2** - Advanced custom components (This notebook!)\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- **Vision Transformers (ViT)** - Transformers for images\n",
        "- **Diffusion Models** - State-of-the-art generative AI\n",
        "- **Neural Architecture Search** - Automated model design\n",
        "- **Quantization & Pruning** - Model optimization for deployment\n",
        "- **TensorFlow Extended (TFX)** - Production ML pipelines\n",
        "\n",
        "---\n",
        "\n",
        "*\"The more you understand the primitives, the better you can innovate.\"*\n",
        "\n",
        "**Happy Deep Learning!**"
      ],
      "metadata": {
        "id": "wQs6OOHGWcDA"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}