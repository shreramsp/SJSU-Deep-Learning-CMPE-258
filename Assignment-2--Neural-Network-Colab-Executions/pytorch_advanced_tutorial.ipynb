{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsPHwagS130i"
      },
      "source": [
        "# PyTorch Advanced: Custom Layers and Deep Architectures\n",
        "\n",
        "## Part 2: Building Complex Operations from Scratch\n",
        "\n",
        "---\n",
        "\n",
        "In Part 1, we learned PyTorch fundamentals, autograd basics, and the high-level nn.Module API. Now we go deeper!\n",
        "\n",
        "### What You'll Learn\n",
        "\n",
        "| Part | Topic | Key Concepts |\n",
        "|------|-------|-------------|\n",
        "| **I** | Advanced Autograd | Nested autograd, Jacobians, custom gradients |\n",
        "| **II** | Building Ops from Scratch | Convolution, pooling, normalization by hand |\n",
        "| **III** | Custom Layers (Primitives) | Build layers using only torch.nn.Parameter |\n",
        "| **IV** | Custom nn.Module Layers | Proper subclassing with forward() |\n",
        "| **V** | Advanced Architectures | Residual blocks, attention, custom normalizations |\n",
        "| **VI** | Custom Training Loops | Full control over training |\n",
        "| **VII** | Practical Demos | Real-world examples with custom components |\n",
        "\n",
        "---\n",
        "\n",
        "*\"To understand the framework, build it from scratch.\"*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4YYRNzV130l",
        "outputId": "31dd238a-78e5-4c5e-d737-5a605d8f60c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version: 2.9.0+cpu\n",
            "CUDA Available:  False\n",
            "\n",
            "Using device: cpu\n",
            "\n",
            "Ready for Advanced PyTorch!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                           SETUP & IMPORTS\n",
        "# ============================================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Tuple, Optional, Callable, Union\n",
        "\n",
        "# Beautiful plots\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "# Check versions and GPU\n",
        "print(f\"PyTorch Version: {torch.__version__}\")\n",
        "print(f\"CUDA Available:  {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Device:      {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"\\nUsing device: {device}\")\n",
        "\n",
        "# Reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"\\nReady for Advanced PyTorch!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbRlppXM130l"
      },
      "source": [
        "---\n",
        "\n",
        "# Part I: Advanced Autograd Patterns\n",
        "\n",
        "## Beyond Basic Gradient Computation\n",
        "\n",
        "In Part 1, we used autograd for simple gradients. Now we'll explore:\n",
        "\n",
        "- **Nested autograd** for higher-order derivatives\n",
        "- **Jacobian and Hessian** computation\n",
        "- **Custom gradients** with torch.autograd.Function\n",
        "- **Gradient hooks** for manipulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUeC9-Kg130m",
        "outputId": "9161f94b-7757-44c4-c837-20527356ad13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "       NESTED AUTOGRAD: HIGHER-ORDER DERIVATIVES\n",
            "============================================================\n",
            "\n",
            "f(x) = x^4, evaluated at x = 2.0\n",
            "\n",
            "f(x)    = 16.0         (expected: 16)\n",
            "f'(x)   = 32.0         (expected: 32 = 4*8)\n",
            "f''(x)  = 48.0         (expected: 48 = 12*4)\n",
            "f'''(x) = 48.0         (expected: 48 = 24*2)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    NESTED AUTOGRAD: HIGHER-ORDER DERIVATIVES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"       NESTED AUTOGRAD: HIGHER-ORDER DERIVATIVES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Example: Compute first, second, and third derivatives\n",
        "# f(x) = x^4\n",
        "# f'(x) = 4x^3\n",
        "# f''(x) = 12x^2\n",
        "# f'''(x) = 24x\n",
        "\n",
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "\n",
        "# First derivative\n",
        "y = x ** 4\n",
        "dy_dx = torch.autograd.grad(y, x, create_graph=True)[0]\n",
        "\n",
        "# Second derivative (need create_graph=True to continue differentiating)\n",
        "d2y_dx2 = torch.autograd.grad(dy_dx, x, create_graph=True)[0]\n",
        "\n",
        "# Third derivative\n",
        "d3y_dx3 = torch.autograd.grad(d2y_dx2, x)[0]\n",
        "\n",
        "print(f\"\\nf(x) = x^4, evaluated at x = {x.item()}\")\n",
        "print(f\"\")\n",
        "print(f\"f(x)    = {y.item():.1f}         (expected: 16)\")\n",
        "print(f\"f'(x)   = {dy_dx.item():.1f}         (expected: 32 = 4*8)\")\n",
        "print(f\"f''(x)  = {d2y_dx2.item():.1f}         (expected: 48 = 12*4)\")\n",
        "print(f\"f'''(x) = {d3y_dx3.item():.1f}         (expected: 48 = 24*2)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CSvHZ-3130m",
        "outputId": "f0fd535a-5e7d-4b9a-c997-010f80b31a95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "              JACOBIAN COMPUTATION\n",
            "============================================================\n",
            "\n",
            "x = [1.0, 2.0, 3.0]\n",
            "y = f(x) = [x1^2, x1*x2, sin(x3)] = [1.0, 2.0, 0.14112000167369843]\n",
            "\n",
            "Jacobian (3x3):\n",
            "[[ 2.         0.         0.       ]\n",
            " [ 2.         1.         0.       ]\n",
            " [ 0.         0.        -0.9899925]]\n",
            "\n",
            "Expected Jacobian:\n",
            "  [2*x1,   0,      0   ]   = [2,   0,     0     ]\n",
            "  [x2,     x1,     0   ]   = [2,   1,     0     ]\n",
            "  [0,      0,  cos(x3) ]   = [0,   0,  -0.9900]\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    JACOBIAN COMPUTATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"              JACOBIAN COMPUTATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# The Jacobian is the matrix of all first-order partial derivatives\n",
        "# For f: R^n -> R^m, the Jacobian J is m x n where J[i,j] = df_i/dx_j\n",
        "\n",
        "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
        "\n",
        "# Vector function: f(x) = [x1^2, x1*x2, sin(x3)]\n",
        "def f(x):\n",
        "    return torch.stack([\n",
        "        x[0] ** 2,\n",
        "        x[0] * x[1],\n",
        "        torch.sin(x[2])\n",
        "    ])\n",
        "\n",
        "# Compute full Jacobian using torch.autograd.functional.jacobian\n",
        "jacobian = torch.autograd.functional.jacobian(f, x)\n",
        "\n",
        "print(f\"\\nx = {x.tolist()}\")\n",
        "print(f\"y = f(x) = [x1^2, x1*x2, sin(x3)] = {f(x).tolist()}\")\n",
        "print(f\"\\nJacobian (3x3):\")\n",
        "print(f\"{jacobian.numpy()}\")\n",
        "\n",
        "print(f\"\\nExpected Jacobian:\")\n",
        "print(f\"  [2*x1,   0,      0   ]   = [2,   0,     0     ]\")\n",
        "print(f\"  [x2,     x1,     0   ]   = [2,   1,     0     ]\")\n",
        "print(f\"  [0,      0,  cos(x3) ]   = [0,   0,  {np.cos(3):.4f}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmVwG5QM130m",
        "outputId": "77c23fa7-b4ab-4d48-b55f-0bb85b8cbcf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "              HESSIAN COMPUTATION\n",
            "============================================================\n",
            "\n",
            "f(x, y) = x^2*y + y^3, at (x, y) = (1.0, 2.0)\n",
            "f = 10.0\n",
            "\n",
            "Gradient: [4.0, 13.0]\n",
            "  Expected: [2xy, x^2 + 3y^2] = [4, 13]\n",
            "\n",
            "Hessian:\n",
            "[[ 4.  2.]\n",
            " [ 2. 12.]]\n",
            "  Expected:\n",
            "  [2y,  2x ]   = [4, 2]\n",
            "  [2x,  6y ]   = [2, 12]\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    HESSIAN COMPUTATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"              HESSIAN COMPUTATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# The Hessian is the matrix of second-order partial derivatives\n",
        "# H[i,j] = d^2f / (dx_i dx_j)\n",
        "\n",
        "x = torch.tensor([1.0, 2.0], requires_grad=True)\n",
        "\n",
        "# Scalar function: f(x, y) = x^2*y + y^3\n",
        "def scalar_fn(x):\n",
        "    return x[0]**2 * x[1] + x[1]**3\n",
        "\n",
        "# Compute Hessian using torch.autograd.functional.hessian\n",
        "hessian = torch.autograd.functional.hessian(scalar_fn, x)\n",
        "\n",
        "# Also compute gradient for reference\n",
        "f_val = scalar_fn(x)\n",
        "grad = torch.autograd.grad(f_val, x, create_graph=True)[0]\n",
        "\n",
        "print(f\"\\nf(x, y) = x^2*y + y^3, at (x, y) = ({x[0].item()}, {x[1].item()})\")\n",
        "print(f\"f = {f_val.item()}\")\n",
        "print(f\"\\nGradient: {grad.tolist()}\")\n",
        "print(f\"  Expected: [2xy, x^2 + 3y^2] = [4, 13]\")\n",
        "print(f\"\\nHessian:\")\n",
        "print(f\"{hessian.numpy()}\")\n",
        "print(f\"  Expected:\")\n",
        "print(f\"  [2y,  2x ]   = [4, 2]\")\n",
        "print(f\"  [2x,  6y ]   = [2, 12]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1mZBZRH130m",
        "outputId": "28c1f41d-334c-4612-ef64-101a3899a5f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "           CUSTOM AUTOGRAD FUNCTIONS\n",
            "============================================================\n",
            "\n",
            "Input: [3.0, 4.0]\n",
            "Gradient (clipped to norm 1.0): [0.7071067690849304, 0.7071067690849304]\n",
            "Gradient norm: 1.0000\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    CUSTOM AUTOGRAD FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"           CUSTOM AUTOGRAD FUNCTIONS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# torch.autograd.Function allows you to define custom forward and backward passes\n",
        "# This is the PyTorch equivalent of TensorFlow's @tf.custom_gradient\n",
        "\n",
        "class ClipGradientNorm(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    Forward: identity function\n",
        "    Backward: clip gradient norm\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, clip_value):\n",
        "        # Save clip_value for backward\n",
        "        ctx.clip_value = clip_value\n",
        "        return x.clone()\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        # Clip the incoming gradient\n",
        "        norm = grad_output.norm()\n",
        "        if norm > ctx.clip_value:\n",
        "            grad_output = grad_output * ctx.clip_value / norm\n",
        "        # Return gradients for x and None for clip_value (not differentiable)\n",
        "        return grad_output, None\n",
        "\n",
        "# Convenience function\n",
        "def clip_gradient_norm(x, clip_value=1.0):\n",
        "    return ClipGradientNorm.apply(x, clip_value)\n",
        "\n",
        "# Test custom gradient\n",
        "x = torch.tensor([3.0, 4.0], requires_grad=True)  # Gradient will have norm 5\n",
        "\n",
        "y = clip_gradient_norm(x, clip_value=1.0)\n",
        "loss = y.sum()\n",
        "loss.backward()\n",
        "\n",
        "print(f\"\\nInput: {x.tolist()}\")\n",
        "print(f\"Gradient (clipped to norm 1.0): {x.grad.tolist()}\")\n",
        "print(f\"Gradient norm: {x.grad.norm().item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NIdVGsk130n",
        "outputId": "881a513f-b32c-4f85-fddc-22fcd980d6fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "         STRAIGHT-THROUGH ESTIMATOR\n",
            "============================================================\n",
            "\n",
            "Input:   [0.30000001192092896, 0.699999988079071, 1.2000000476837158, 2.5]\n",
            "Rounded: [0.0, 1.0, 1.0, 2.0]\n",
            "Gradient (straight-through): [0.0, 2.0, 2.0, 4.0]\n",
            "\n",
            " Note: Round is non-differentiable, but we can still train!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    STRAIGHT-THROUGH ESTIMATOR\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"         STRAIGHT-THROUGH ESTIMATOR\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# The straight-through estimator is used for:\n",
        "# - Binary/discrete operations that are non-differentiable\n",
        "# - Quantization in neural networks\n",
        "\n",
        "class StraightThroughRound(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    Forward: round to nearest integer\n",
        "    Backward: pass gradient through unchanged (identity)\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def forward(ctx, x):\n",
        "        return torch.round(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return grad_output  # Straight-through: gradient = identity\n",
        "\n",
        "class StraightThroughSign(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    Forward: sign function (-1, 0, or 1)\n",
        "    Backward: gradient of hard tanh (1 if |x| <= 1, else 0)\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def forward(ctx, x):\n",
        "        ctx.save_for_backward(x)\n",
        "        return torch.sign(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        x, = ctx.saved_tensors\n",
        "        # Gradient is 1 where |x| <= 1, 0 elsewhere\n",
        "        return grad_output * (x.abs() <= 1).float()\n",
        "\n",
        "# Convenience functions\n",
        "straight_through_round = StraightThroughRound.apply\n",
        "straight_through_sign = StraightThroughSign.apply\n",
        "\n",
        "# Test\n",
        "x = torch.tensor([0.3, 0.7, 1.2, 2.5], requires_grad=True)\n",
        "\n",
        "y = straight_through_round(x)\n",
        "loss = (y ** 2).sum()\n",
        "loss.backward()\n",
        "\n",
        "print(f\"\\nInput:   {x.tolist()}\")\n",
        "print(f\"Rounded: {y.tolist()}\")\n",
        "print(f\"Gradient (straight-through): {x.grad.tolist()}\")\n",
        "print(f\"\\n Note: Round is non-differentiable, but we can still train!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAQA7S-x130n",
        "outputId": "b40299bf-694a-456d-c7ce-9910e114e06d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "               GRADIENT HOOKS\n",
            "============================================================\n",
            "\n",
            "Gradient logged:\n",
            "  Mean: 0.3841\n",
            "  Std:  1.7298\n",
            "  Norm: 5.3297\n",
            "\n",
            " Hooks are powerful for:\n",
            "  - Debugging gradient flow\n",
            "  - Gradient clipping per-tensor\n",
            "  - Feature visualization (GradCAM)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    GRADIENT HOOKS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"               GRADIENT HOOKS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Hooks allow you to inspect or modify gradients during backward pass\n",
        "# register_hook() on tensors, register_backward_hook() on modules\n",
        "\n",
        "gradient_log = []\n",
        "\n",
        "def gradient_logger_hook(grad):\n",
        "    \"\"\"Hook that logs gradient statistics.\"\"\"\n",
        "    gradient_log.append({\n",
        "        'mean': grad.mean().item(),\n",
        "        'std': grad.std().item(),\n",
        "        'norm': grad.norm().item()\n",
        "    })\n",
        "    return grad  # Return unchanged gradient\n",
        "\n",
        "def gradient_clip_hook(max_norm):\n",
        "    \"\"\"Create a hook that clips gradient norm.\"\"\"\n",
        "    def hook(grad):\n",
        "        norm = grad.norm()\n",
        "        if norm > max_norm:\n",
        "            return grad * max_norm / norm\n",
        "        return grad\n",
        "    return hook\n",
        "\n",
        "# Example: Log gradients during training\n",
        "x = torch.randn(10, requires_grad=True)\n",
        "handle = x.register_hook(gradient_logger_hook)\n",
        "\n",
        "# Forward and backward\n",
        "y = (x ** 2).sum()\n",
        "y.backward()\n",
        "\n",
        "print(f\"\\nGradient logged:\")\n",
        "print(f\"  Mean: {gradient_log[-1]['mean']:.4f}\")\n",
        "print(f\"  Std:  {gradient_log[-1]['std']:.4f}\")\n",
        "print(f\"  Norm: {gradient_log[-1]['norm']:.4f}\")\n",
        "\n",
        "# Remove hook when done\n",
        "handle.remove()\n",
        "\n",
        "print(\"\\n Hooks are powerful for:\")\n",
        "print(\"  - Debugging gradient flow\")\n",
        "print(\"  - Gradient clipping per-tensor\")\n",
        "print(\"  - Feature visualization (GradCAM)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sh3MJQUZ130n",
        "outputId": "a21b38f0-5fa3-4fc7-d853-94c62f84dfc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "            GRADIENT ACCUMULATION\n",
            "============================================================\n",
            "\n",
            "Gradient Accumulation Pattern:\n",
            "  1. optimizer.zero_grad() once at start\n",
            "  2. Scale loss by 1/accumulation_steps\n",
            "  3. loss.backward() accumulates gradients\n",
            "  4. optimizer.step() after all steps\n",
            "  5. Effective batch = mini_batch * N\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    GRADIENT ACCUMULATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"            GRADIENT ACCUMULATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Gradient accumulation is useful when:\n",
        "# - Batch size is too large for GPU memory\n",
        "# - You want effective larger batch sizes\n",
        "\n",
        "def train_with_accumulation(model, data, labels, batch_size, accumulation_steps, optimizer, loss_fn):\n",
        "    \"\"\"\n",
        "    Train with gradient accumulation.\n",
        "    Effective batch size = batch_size * accumulation_steps\n",
        "    \"\"\"\n",
        "    n_samples = len(data)\n",
        "    model.train()\n",
        "\n",
        "    # Zero gradients once at the start\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for step in range(accumulation_steps):\n",
        "        # Get mini-batch\n",
        "        start = (step * batch_size) % n_samples\n",
        "        end = start + batch_size\n",
        "        x_batch = data[start:end]\n",
        "        y_batch = labels[start:end]\n",
        "\n",
        "        # Forward pass\n",
        "        predictions = model(x_batch)\n",
        "        loss = loss_fn(predictions, y_batch)\n",
        "\n",
        "        # Scale loss by accumulation steps (to average gradients)\n",
        "        loss = loss / accumulation_steps\n",
        "\n",
        "        # Backward pass - gradients accumulate!\n",
        "        loss.backward()\n",
        "\n",
        "    # Apply accumulated gradients\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item() * accumulation_steps  # Return unscaled loss\n",
        "\n",
        "print(\"\\nGradient Accumulation Pattern:\")\n",
        "print(\"  1. optimizer.zero_grad() once at start\")\n",
        "print(\"  2. Scale loss by 1/accumulation_steps\")\n",
        "print(\"  3. loss.backward() accumulates gradients\")\n",
        "print(\"  4. optimizer.step() after all steps\")\n",
        "print(\"  5. Effective batch = mini_batch * N\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDFNAZ2Z130o"
      },
      "source": [
        "---\n",
        "\n",
        "# Part II: Building Operations from Scratch\n",
        "\n",
        "## Understanding Neural Network Primitives\n",
        "\n",
        "Before using nn.Module layers, let's understand what they do by building them ourselves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xW8lDgQj130o",
        "outputId": "ad6a308d-adbf-45a5-b894-73fe1c85287e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "           CONVOLUTION FROM SCRATCH\n",
            "============================================================\n",
            "\n",
            "Input shape:  torch.Size([1, 1, 5, 5])\n",
            "Kernel shape: torch.Size([2, 1, 3, 3])\n",
            "Output shape: torch.Size([1, 2, 3, 3])\n",
            "Matches F.conv2d: True\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    CONVOLUTION FROM SCRATCH\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"           CONVOLUTION FROM SCRATCH\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def conv2d_naive(input_tensor, kernel, stride=1, padding=0):\n",
        "    \"\"\"\n",
        "    Naive 2D convolution implementation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_tensor : tensor (batch, in_channels, height, width)\n",
        "    kernel : tensor (out_channels, in_channels, kernel_h, kernel_w)\n",
        "    stride : int\n",
        "    padding : int\n",
        "    \"\"\"\n",
        "    batch_size, in_channels, in_h, in_w = input_tensor.shape\n",
        "    out_channels, _, k_h, k_w = kernel.shape\n",
        "\n",
        "    # Apply padding\n",
        "    if padding > 0:\n",
        "        input_tensor = F.pad(input_tensor, [padding] * 4)\n",
        "        in_h += 2 * padding\n",
        "        in_w += 2 * padding\n",
        "\n",
        "    out_h = (in_h - k_h) // stride + 1\n",
        "    out_w = (in_w - k_w) // stride + 1\n",
        "\n",
        "    output = torch.zeros(batch_size, out_channels, out_h, out_w)\n",
        "\n",
        "    for i in range(out_h):\n",
        "        for j in range(out_w):\n",
        "            h_start = i * stride\n",
        "            w_start = j * stride\n",
        "            # Extract patch: (batch, in_channels, k_h, k_w)\n",
        "            patch = input_tensor[:, :, h_start:h_start+k_h, w_start:w_start+k_w]\n",
        "\n",
        "            # Convolve: sum over (in_channels, k_h, k_w)\n",
        "            # patch: (batch, in_c, k_h, k_w)\n",
        "            # kernel: (out_c, in_c, k_h, k_w)\n",
        "            # output: (batch, out_c)\n",
        "            conv = torch.einsum('bihw,oihw->bo', patch, kernel)\n",
        "            output[:, :, i, j] = conv\n",
        "\n",
        "    return output\n",
        "\n",
        "# Test our implementation\n",
        "x = torch.randn(1, 1, 5, 5)  # 1 image, 1 channel, 5x5\n",
        "kernel = torch.randn(2, 1, 3, 3)  # 2 output channels, 1 input channel, 3x3\n",
        "\n",
        "our_output = conv2d_naive(x, kernel, stride=1, padding=0)\n",
        "torch_output = F.conv2d(x, kernel, stride=1, padding=0)\n",
        "\n",
        "print(f\"\\nInput shape:  {x.shape}\")\n",
        "print(f\"Kernel shape: {kernel.shape}\")\n",
        "print(f\"Output shape: {our_output.shape}\")\n",
        "print(f\"Matches F.conv2d: {torch.allclose(our_output, torch_output, atol=1e-5)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoqcFXo3130o",
        "outputId": "f13a5ba1-119f-4a88-c7c0-f80b5ee76bea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "           MAX POOLING FROM SCRATCH\n",
            "============================================================\n",
            "\n",
            "Input shape: torch.Size([1, 1, 4, 4])\n",
            "Input:\n",
            "[[ 1.  2.  3.  4.]\n",
            " [ 5.  6.  7.  8.]\n",
            " [ 9. 10. 11. 12.]\n",
            " [13. 14. 15. 16.]]\n",
            "\n",
            "Output shape: torch.Size([1, 1, 2, 2])\n",
            "Output:\n",
            "[[ 6.  8.]\n",
            " [14. 16.]]\n",
            "\n",
            "Matches F.max_pool2d: True\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    MAX POOLING FROM SCRATCH\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"           MAX POOLING FROM SCRATCH\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def max_pool2d_naive(input_tensor, pool_size=2, stride=2):\n",
        "    \"\"\"\n",
        "    Naive max pooling implementation.\n",
        "\n",
        "    For each pool_size x pool_size window, take the maximum.\n",
        "    \"\"\"\n",
        "    batch_size, channels, in_h, in_w = input_tensor.shape\n",
        "\n",
        "    out_h = (in_h - pool_size) // stride + 1\n",
        "    out_w = (in_w - pool_size) // stride + 1\n",
        "\n",
        "    output = torch.zeros(batch_size, channels, out_h, out_w)\n",
        "\n",
        "    for i in range(out_h):\n",
        "        for j in range(out_w):\n",
        "            h_start = i * stride\n",
        "            w_start = j * stride\n",
        "            # Extract window\n",
        "            window = input_tensor[:, :, h_start:h_start+pool_size,\n",
        "                                  w_start:w_start+pool_size]\n",
        "            # Max over spatial dimensions\n",
        "            output[:, :, i, j] = window.amax(dim=(2, 3))\n",
        "\n",
        "    return output\n",
        "\n",
        "# Test\n",
        "x = torch.tensor([[[[1., 2., 3., 4.],\n",
        "                    [5., 6., 7., 8.],\n",
        "                    [9., 10., 11., 12.],\n",
        "                    [13., 14., 15., 16.]]]])\n",
        "\n",
        "print(f\"\\nInput shape: {x.shape}\")\n",
        "print(f\"Input:\")\n",
        "print(x[0, 0].numpy())\n",
        "\n",
        "our_pool = max_pool2d_naive(x, pool_size=2, stride=2)\n",
        "torch_pool = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "\n",
        "print(f\"\\nOutput shape: {our_pool.shape}\")\n",
        "print(f\"Output:\")\n",
        "print(our_pool[0, 0].numpy())\n",
        "print(f\"\\nMatches F.max_pool2d: {torch.allclose(our_pool, torch_pool)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLm27c7q130o",
        "outputId": "cfe4ed26-303b-4a7a-e3ce-9e1044b2e109"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "        BATCH NORMALIZATION FROM SCRATCH\n",
            "============================================================\n",
            "\n",
            "Input shape: torch.Size([8, 4])\n",
            "Input mean per feature: [0.5206222534179688, -0.17970936000347137, 0.45833995938301086, 0.2649056315422058]\n",
            "Input std per feature:  [0.3801516592502594, 1.219132661819458, 0.4272221326828003, 0.585858941078186]\n",
            "\n",
            "Output (training) mean: ['-0.0000', '0.0000', '0.0000', '0.0000']\n",
            "Output (training) std:  ['1.0690', '1.0690', '1.0690', '1.0690']\n",
            "\n",
            " After BatchNorm, each feature has ~0 mean and ~1 std!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    BATCH NORMALIZATION FROM SCRATCH\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"        BATCH NORMALIZATION FROM SCRATCH\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class BatchNormFromScratch:\n",
        "    \"\"\"\n",
        "    Batch Normalization implemented from scratch.\n",
        "\n",
        "    During training:\n",
        "        x_norm = (x - batch_mean) / sqrt(batch_var + epsilon)\n",
        "        y = gamma * x_norm + beta\n",
        "\n",
        "    During inference:\n",
        "        Use running mean and variance instead of batch statistics.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_features, epsilon=1e-5, momentum=0.1):\n",
        "        self.epsilon = epsilon\n",
        "        self.momentum = momentum\n",
        "\n",
        "        # Learnable parameters\n",
        "        self.gamma = torch.ones(num_features, requires_grad=True)\n",
        "        self.beta = torch.zeros(num_features, requires_grad=True)\n",
        "\n",
        "        # Running statistics (not trainable)\n",
        "        self.running_mean = torch.zeros(num_features)\n",
        "        self.running_var = torch.ones(num_features)\n",
        "\n",
        "    def __call__(self, x, training=True):\n",
        "        if training:\n",
        "            # Compute batch statistics\n",
        "            batch_mean = x.mean(dim=0)\n",
        "            batch_var = x.var(dim=0, unbiased=False)\n",
        "\n",
        "            # Update running statistics (detached, no gradient)\n",
        "            with torch.no_grad():\n",
        "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean\n",
        "                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var\n",
        "\n",
        "            mean, var = batch_mean, batch_var\n",
        "        else:\n",
        "            mean, var = self.running_mean, self.running_var\n",
        "\n",
        "        # Normalize\n",
        "        x_norm = (x - mean) / torch.sqrt(var + self.epsilon)\n",
        "\n",
        "        # Scale and shift\n",
        "        return self.gamma * x_norm + self.beta\n",
        "\n",
        "    def parameters(self):\n",
        "        return [self.gamma, self.beta]\n",
        "\n",
        "# Test\n",
        "bn = BatchNormFromScratch(num_features=4)\n",
        "x = torch.randn(8, 4)  # Batch of 8, 4 features\n",
        "\n",
        "y_train = bn(x, training=True)\n",
        "y_eval = bn(x, training=False)\n",
        "\n",
        "print(f\"\\nInput shape: {x.shape}\")\n",
        "print(f\"Input mean per feature: {x.mean(dim=0).tolist()}\")\n",
        "print(f\"Input std per feature:  {x.std(dim=0).tolist()}\")\n",
        "print(f\"\\nOutput (training) mean: {[f'{v:.4f}' for v in y_train.mean(dim=0).tolist()]}\")\n",
        "print(f\"Output (training) std:  {[f'{v:.4f}' for v in y_train.std(dim=0).tolist()]}\")\n",
        "print(f\"\\n After BatchNorm, each feature has ~0 mean and ~1 std!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0QylO2n130o",
        "outputId": "daaa8441-1060-440a-a439-fc3574e7f9a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "        LAYER NORMALIZATION FROM SCRATCH\n",
            "============================================================\n",
            "\n",
            "Input shape: torch.Size([2, 3, 4])\n",
            "\n",
            "For sample [0, 0, :]:\n",
            "  Input:  [-0.16911523044109344, 1.931160569190979, 1.0118638277053833, -1.4364064931869507]\n",
            "  Output: ['-0.3981', '1.2626', '0.5357', '-1.4002']\n",
            "  Output mean: 0.000000\n",
            "  Output std:  1.1547\n",
            "\n",
            " Key difference:\n",
            "  BatchNorm: normalize across batch (for each feature)\n",
            "  LayerNorm: normalize across features (for each sample)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    LAYER NORMALIZATION FROM SCRATCH\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"        LAYER NORMALIZATION FROM SCRATCH\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class LayerNormFromScratch:\n",
        "    \"\"\"\n",
        "    Layer Normalization: Normalize across features (not batch).\n",
        "\n",
        "    Used in Transformers because:\n",
        "    - Works with any batch size (including 1)\n",
        "    - No running statistics needed\n",
        "    - Each sample normalized independently\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, normalized_shape, epsilon=1e-5):\n",
        "        self.epsilon = epsilon\n",
        "        self.normalized_shape = normalized_shape\n",
        "\n",
        "        # Learnable parameters\n",
        "        self.gamma = torch.ones(normalized_shape, requires_grad=True)\n",
        "        self.beta = torch.zeros(normalized_shape, requires_grad=True)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        # Compute statistics across last dimensions\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "\n",
        "        # Normalize\n",
        "        x_norm = (x - mean) / torch.sqrt(var + self.epsilon)\n",
        "\n",
        "        # Scale and shift\n",
        "        return self.gamma * x_norm + self.beta\n",
        "\n",
        "    def parameters(self):\n",
        "        return [self.gamma, self.beta]\n",
        "\n",
        "# Test\n",
        "ln = LayerNormFromScratch(normalized_shape=4)\n",
        "x = torch.randn(2, 3, 4)  # (batch, seq, features)\n",
        "\n",
        "y = ln(x)\n",
        "\n",
        "print(f\"\\nInput shape: {x.shape}\")\n",
        "print(f\"\\nFor sample [0, 0, :]:\")\n",
        "print(f\"  Input:  {x[0, 0, :].tolist()}\")\n",
        "print(f\"  Output: {[f'{v:.4f}' for v in y[0, 0, :].tolist()]}\")\n",
        "print(f\"  Output mean: {y[0, 0, :].mean().item():.6f}\")\n",
        "print(f\"  Output std:  {y[0, 0, :].std().item():.4f}\")\n",
        "\n",
        "print(f\"\\n Key difference:\")\n",
        "print(f\"  BatchNorm: normalize across batch (for each feature)\")\n",
        "print(f\"  LayerNorm: normalize across features (for each sample)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlXqtm1V130o",
        "outputId": "5c4f54ff-6074-4ea6-b7c2-9a032c1cee17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "            DROPOUT FROM SCRATCH\n",
            "============================================================\n",
            "Input: all ones, shape torch.Size([2, 10])\n",
            "\n",
            "Dropout sample 1: [0.0, 0.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0, 2.0, 2.0]\n",
            "Dropout sample 2: [2.0, 2.0, 0.0, 2.0, 2.0, 0.0, 0.0, 2.0, 2.0, 0.0]\n",
            "Dropout sample 3: [2.0, 2.0, 0.0, 0.0, 0.0, 2.0, 2.0, 2.0, 0.0, 0.0]\n",
            "\n",
            "During inference (training=False):\n",
            "  Output: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "\n",
            "Average over 1000 samples: 1.0055 (should be ~1.0)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    DROPOUT FROM SCRATCH\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"            DROPOUT FROM SCRATCH\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def dropout_from_scratch(x, rate=0.5, training=True):\n",
        "    \"\"\"\n",
        "    Dropout: Randomly zero out neurons during training.\n",
        "\n",
        "    Key insight: Scale by 1/(1-rate) during training so that\n",
        "    expected value remains the same during inference.\n",
        "    \"\"\"\n",
        "    if not training or rate == 0:\n",
        "        return x\n",
        "\n",
        "    # Create random mask\n",
        "    keep_prob = 1 - rate\n",
        "    mask = (torch.rand_like(x) < keep_prob).float()\n",
        "\n",
        "    # Apply mask and scale (inverted dropout)\n",
        "    return (x * mask) / keep_prob\n",
        "\n",
        "# Test\n",
        "x = torch.ones(2, 10)\n",
        "\n",
        "print(f\"Input: all ones, shape {x.shape}\")\n",
        "print(f\"\")\n",
        "\n",
        "# Multiple dropout samples\n",
        "for i in range(3):\n",
        "    dropped = dropout_from_scratch(x, rate=0.5, training=True)\n",
        "    print(f\"Dropout sample {i+1}: {dropped[0].tolist()}\")\n",
        "\n",
        "print(f\"\\nDuring inference (training=False):\")\n",
        "print(f\"  Output: {dropout_from_scratch(x, rate=0.5, training=False)[0].tolist()}\")\n",
        "\n",
        "# Verify expected value is preserved\n",
        "samples = torch.stack([dropout_from_scratch(x, rate=0.5) for _ in range(1000)])\n",
        "print(f\"\\nAverage over 1000 samples: {samples.mean().item():.4f} (should be ~1.0)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eXTxywF130o"
      },
      "source": [
        "---\n",
        "\n",
        "# Part III: Custom Layers Using Primitives\n",
        "\n",
        "## Building Layers with nn.Parameter Only\n",
        "\n",
        "Before using PyTorch's layer classes, let's build fully functional layers using only basic operations. This shows exactly what happens under the hood."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVIR_S55130o",
        "outputId": "cd252757-310c-49dc-a392-5850dd3dbc73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "          DENSE LAYER FROM PRIMITIVES\n",
            "============================================================\n",
            "\n",
            "DenseLayerPrimitive(4, 3, activation='relu')\n",
            "Input shape:  torch.Size([2, 4])\n",
            "Output shape: torch.Size([2, 3])\n",
            "Weight shape: torch.Size([4, 3])\n",
            "Bias shape:   torch.Size([3])\n",
            "\n",
            "Output:\n",
            "[[2.5953686 0.        2.304854 ]\n",
            " [0.        0.6638745 0.       ]]\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    DENSE LAYER FROM PRIMITIVES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"          DENSE LAYER FROM PRIMITIVES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class DenseLayerPrimitive:\n",
        "    \"\"\"\n",
        "    Fully connected layer using only basic PyTorch operations.\n",
        "\n",
        "    Mathematically: y = activation(x @ W + b)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, activation=None, use_bias=True):\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "        # Select activation\n",
        "        self.activation = {\n",
        "            None: lambda x: x,\n",
        "            'relu': F.relu,\n",
        "            'sigmoid': torch.sigmoid,\n",
        "            'tanh': torch.tanh,\n",
        "            'softmax': lambda x: F.softmax(x, dim=-1)\n",
        "        }.get(activation, activation)  # Allow passing functions directly\n",
        "\n",
        "        # He initialization for weights\n",
        "        stddev = np.sqrt(2.0 / in_features)\n",
        "        self.W = torch.randn(in_features, out_features, requires_grad=True) * stddev\n",
        "\n",
        "        if use_bias:\n",
        "            self.b = torch.zeros(out_features, requires_grad=True)\n",
        "        else:\n",
        "            self.b = None\n",
        "\n",
        "    def __call__(self, x):\n",
        "        \"\"\"Forward pass: y = activation(x @ W + b)\"\"\"\n",
        "        out = x @ self.W\n",
        "        if self.use_bias:\n",
        "            out = out + self.b\n",
        "        return self.activation(out)\n",
        "\n",
        "    def parameters(self):\n",
        "        if self.use_bias:\n",
        "            return [self.W, self.b]\n",
        "        return [self.W]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"DenseLayerPrimitive({self.in_features}, {self.out_features})\"\n",
        "\n",
        "# Test\n",
        "dense = DenseLayerPrimitive(4, 3, activation='relu')\n",
        "x = torch.randn(2, 4)\n",
        "y = dense(x)\n",
        "\n",
        "print(f\"\\nDenseLayerPrimitive(4, 3, activation='relu')\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"Weight shape: {dense.W.shape}\")\n",
        "print(f\"Bias shape:   {dense.b.shape}\")\n",
        "print(f\"\\nOutput:\\n{y.detach().numpy()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uM5Lo_Ci130p",
        "outputId": "cd4c088c-eade-4be0-aea0-3146d48b31fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "          CONV2D LAYER FROM PRIMITIVES\n",
            "============================================================\n",
            "\n",
            "Conv2DLayerPrimitive(3, 16, kernel_size=3, padding=1)\n",
            "Input shape:  torch.Size([1, 3, 28, 28])\n",
            "Output shape: torch.Size([1, 16, 28, 28])\n",
            "Kernel shape: torch.Size([16, 3, 3, 3])\n",
            "Parameters:   448\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    CONV2D LAYER FROM PRIMITIVES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"          CONV2D LAYER FROM PRIMITIVES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class Conv2DLayerPrimitive:\n",
        "    \"\"\"\n",
        "    2D Convolutional layer using only basic operations and F.conv2d.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size,\n",
        "                 stride=1, padding=0, activation=None, use_bias=True):\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "        # Handle kernel_size as int or tuple\n",
        "        if isinstance(kernel_size, int):\n",
        "            kernel_size = (kernel_size, kernel_size)\n",
        "\n",
        "        # Select activation\n",
        "        self.activation = {\n",
        "            None: lambda x: x,\n",
        "            'relu': F.relu,\n",
        "            'sigmoid': torch.sigmoid,\n",
        "            'tanh': torch.tanh,\n",
        "        }.get(activation, activation)\n",
        "\n",
        "        # He initialization\n",
        "        fan_in = kernel_size[0] * kernel_size[1] * in_channels\n",
        "        stddev = np.sqrt(2.0 / fan_in)\n",
        "\n",
        "        # Kernel shape: (out_channels, in_channels, height, width)\n",
        "        self.kernel = torch.randn(\n",
        "            out_channels, in_channels, kernel_size[0], kernel_size[1],\n",
        "            requires_grad=True\n",
        "        ) * stddev\n",
        "\n",
        "        if use_bias:\n",
        "            self.bias = torch.zeros(out_channels, requires_grad=True)\n",
        "        else:\n",
        "            self.bias = None\n",
        "\n",
        "    def __call__(self, x):\n",
        "        \"\"\"Forward pass using F.conv2d\"\"\"\n",
        "        out = F.conv2d(x, self.kernel, bias=self.bias,\n",
        "                       stride=self.stride, padding=self.padding)\n",
        "        return self.activation(out)\n",
        "\n",
        "    def parameters(self):\n",
        "        if self.use_bias:\n",
        "            return [self.kernel, self.bias]\n",
        "        return [self.kernel]\n",
        "\n",
        "# Test\n",
        "conv = Conv2DLayerPrimitive(3, 16, kernel_size=3, padding=1, activation='relu')\n",
        "x = torch.randn(1, 3, 28, 28)  # 1 image, 3 channels (RGB), 28x28\n",
        "y = conv(x)\n",
        "\n",
        "print(f\"\\nConv2DLayerPrimitive(3, 16, kernel_size=3, padding=1)\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"Kernel shape: {conv.kernel.shape}\")\n",
        "print(f\"Parameters:   {conv.kernel.numel() + conv.bias.numel():,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cz6Uihdn130p",
        "outputId": "579c6ad1-bf74-4152-9b28-c3deb54074d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "          COMPLETE CNN FROM PRIMITIVES\n",
            "============================================================\n",
            "\n",
            "==================================================\n",
            "        CNN FROM PRIMITIVES - SUMMARY\n",
            "==================================================\n",
            "Layer 1: Conv2DLayerPrimitive | Params: 320\n",
            "Layer 2: Conv2DLayerPrimitive | Params: 18,496\n",
            "Layer 3: DenseLayerPrimitive  | Params: 401,536\n",
            "Layer 4: DenseLayerPrimitive  | Params: 1,290\n",
            "--------------------------------------------------\n",
            "Total trainable parameters: 421,642\n",
            "\n",
            "Input shape:  torch.Size([4, 1, 28, 28])\n",
            "Output shape: torch.Size([4, 10])\n",
            "Output sum per sample: [1.0, 1.0, 1.0000001192092896, 1.0]  (should be ~1.0)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    COMPLETE CNN FROM PRIMITIVES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"          COMPLETE CNN FROM PRIMITIVES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class CNNFromPrimitives:\n",
        "    \"\"\"\n",
        "    A complete CNN built using only primitive layers.\n",
        "\n",
        "    Architecture:\n",
        "        Conv(3x3) -> ReLU -> MaxPool -> Conv(3x3) -> ReLU -> MaxPool -> Flatten -> Dense -> Dense\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_shape, num_classes):\n",
        "        self.input_shape = input_shape\n",
        "        in_channels = input_shape[0]  # PyTorch: (C, H, W)\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.conv1 = Conv2DLayerPrimitive(in_channels, 32, kernel_size=3, padding=1, activation='relu')\n",
        "        self.conv2 = Conv2DLayerPrimitive(32, 64, kernel_size=3, padding=1, activation='relu')\n",
        "\n",
        "        # Calculate flattened size after convolutions and pooling\n",
        "        # With same padding and 2x2 pooling twice: H/4, W/4\n",
        "        h, w = input_shape[1] // 4, input_shape[2] // 4\n",
        "        flat_size = h * w * 64\n",
        "\n",
        "        # Dense layers\n",
        "        self.fc1 = DenseLayerPrimitive(flat_size, 128, activation='relu')\n",
        "        self.fc2 = DenseLayerPrimitive(128, num_classes, activation='softmax')\n",
        "\n",
        "        self.layers = [self.conv1, self.conv2, self.fc1, self.fc2]\n",
        "\n",
        "    def __call__(self, x, training=True):\n",
        "        # Conv block 1\n",
        "        x = self.conv1(x)\n",
        "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "\n",
        "        # Conv block 2\n",
        "        x = self.conv2(x)\n",
        "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "\n",
        "        # Flatten\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # Dense layers with dropout\n",
        "        x = self.fc1(x)\n",
        "        if training:\n",
        "            x = dropout_from_scratch(x, rate=0.5, training=True)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def parameters(self):\n",
        "        params = []\n",
        "        for layer in self.layers:\n",
        "            params.extend(layer.parameters())\n",
        "        return params\n",
        "\n",
        "    def summary(self):\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"        CNN FROM PRIMITIVES - SUMMARY\")\n",
        "        print(\"=\"*50)\n",
        "        total = 0\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            params = sum(p.numel() for p in layer.parameters())\n",
        "            total += params\n",
        "            print(f\"Layer {i+1}: {layer.__class__.__name__:20} | Params: {params:,}\")\n",
        "        print(\"-\"*50)\n",
        "        print(f\"Total trainable parameters: {total:,}\")\n",
        "\n",
        "# Create and test\n",
        "cnn = CNNFromPrimitives(input_shape=(1, 28, 28), num_classes=10)\n",
        "cnn.summary()\n",
        "\n",
        "# Test forward pass\n",
        "x = torch.randn(4, 1, 28, 28)\n",
        "y = cnn(x)\n",
        "print(f\"\\nInput shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"Output sum per sample: {y.sum(dim=1).tolist()}  (should be ~1.0)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJ7pcu0G130p"
      },
      "source": [
        "---\n",
        "\n",
        "# Part IV: Custom nn.Module Layers\n",
        "\n",
        "## The Proper Way to Build Custom Layers\n",
        "\n",
        "PyTorch's `nn.Module` provides a clean API for custom layers with:\n",
        "- **Automatic parameter registration** with nn.Parameter\n",
        "- **forward() method** for the forward pass\n",
        "- **Proper device management** with `.to(device)`\n",
        "\n",
        "This gives you all the benefits of primitives PLUS:\n",
        "- Automatic weight tracking via `.parameters()`\n",
        "- Serialization with `state_dict()`\n",
        "- Integration with optimizers and training loops\n",
        "- Proper shape inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RoTZXGj130p",
        "outputId": "6eac6f53-0da6-410a-c2be-5cf5b40174df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "          CUSTOM nn.Module LAYER: BASICS\n",
            "============================================================\n",
            "\n",
            "CustomDenseLayer(16, 32, activation='relu')\n",
            "Input shape:  torch.Size([4, 16])\n",
            "Output shape: torch.Size([4, 32])\n",
            "Weight shape: torch.Size([16, 32])\n",
            "Trainable parameters: 544\n",
            "\n",
            "Model:\n",
            "CustomDenseLayer(\n",
            "  in_features=16, out_features=32, bias=True\n",
            "  (activation): ReLU()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    CUSTOM nn.Module LAYER: BASICS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"          CUSTOM nn.Module LAYER: BASICS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class CustomDenseLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Custom Dense layer demonstrating the nn.Module API.\n",
        "\n",
        "    Key points:\n",
        "    - Inherit from nn.Module\n",
        "    - Call super().__init__() in __init__\n",
        "    - Use nn.Parameter for learnable weights\n",
        "    - Implement forward() method\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, activation=None, use_bias=True):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "        # nn.Parameter automatically registers weights!\n",
        "        self.weight = nn.Parameter(\n",
        "            torch.randn(in_features, out_features) * np.sqrt(2.0 / in_features)\n",
        "        )\n",
        "\n",
        "        if use_bias:\n",
        "            self.bias = nn.Parameter(torch.zeros(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "        # Store activation\n",
        "        self.activation = {\n",
        "            None: nn.Identity(),\n",
        "            'relu': nn.ReLU(),\n",
        "            'sigmoid': nn.Sigmoid(),\n",
        "            'tanh': nn.Tanh()\n",
        "        }.get(activation, activation)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass.\"\"\"\n",
        "        output = x @ self.weight\n",
        "        if self.bias is not None:\n",
        "            output = output + self.bias\n",
        "        return self.activation(output)\n",
        "\n",
        "    def extra_repr(self):\n",
        "        \"\"\"Extra info for print(model).\"\"\"\n",
        "        return f'in_features={self.in_features}, out_features={self.out_features}, bias={self.use_bias}'\n",
        "\n",
        "# Test custom layer\n",
        "custom_dense = CustomDenseLayer(16, 32, activation='relu')\n",
        "x = torch.randn(4, 16)\n",
        "y = custom_dense(x)\n",
        "\n",
        "print(f\"\\nCustomDenseLayer(16, 32, activation='relu')\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"Weight shape: {custom_dense.weight.shape}\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in custom_dense.parameters())}\")\n",
        "print(f\"\\nModel:\")\n",
        "print(custom_dense)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Xlc5mqo130p",
        "outputId": "6284606e-3557-4d73-9a31-82d6df6a0244"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "         CUSTOM LAYER: SELF-ATTENTION\n",
            "============================================================\n",
            "\n",
            "SelfAttentionLayer(embed_dim=64, num_heads=4)\n",
            "Input shape:             torch.Size([2, 10, 64])\n",
            "Output shape:            torch.Size([2, 10, 64])\n",
            "Attention weights shape: torch.Size([2, 4, 10, 10])\n",
            "Trainable parameters:    16,384\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    CUSTOM LAYER: SELF-ATTENTION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"         CUSTOM LAYER: SELF-ATTENTION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class SelfAttentionLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Self-Attention layer (simplified version of Transformer attention).\n",
        "\n",
        "    Attention(Q, K, V) = softmax(Q @ K^T / sqrt(d_k)) @ V\n",
        "\n",
        "    In self-attention, Q, K, V all come from the same input.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_dim, num_heads=1):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
        "\n",
        "        # Linear projections for Q, K, V\n",
        "        self.W_q = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.W_k = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.W_v = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.W_o = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        # Linear projections\n",
        "        Q = self.W_q(x)  # (batch, seq, embed)\n",
        "        K = self.W_k(x)\n",
        "        V = self.W_v(x)\n",
        "\n",
        "        # Reshape for multi-head attention\n",
        "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        # Now: (batch, heads, seq, head_dim)\n",
        "\n",
        "        # Attention scores: Q @ K^T / sqrt(d_k)\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
        "\n",
        "        # Apply mask if provided\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        # Softmax\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "        # Apply attention to values\n",
        "        context = torch.matmul(attention_weights, V)  # (batch, heads, seq, head_dim)\n",
        "\n",
        "        # Reshape back\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)\n",
        "\n",
        "        # Output projection\n",
        "        output = self.W_o(context)\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "# Test\n",
        "attention = SelfAttentionLayer(embed_dim=64, num_heads=4)\n",
        "x = torch.randn(2, 10, 64)  # (batch, seq_len, embed_dim)\n",
        "output, weights = attention(x)\n",
        "\n",
        "print(f\"\\nSelfAttentionLayer(embed_dim=64, num_heads=4)\")\n",
        "print(f\"Input shape:             {x.shape}\")\n",
        "print(f\"Output shape:            {output.shape}\")\n",
        "print(f\"Attention weights shape: {weights.shape}\")\n",
        "print(f\"Trainable parameters:    {sum(p.numel() for p in attention.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOwGtJYQ130p",
        "outputId": "62bc11ce-b75c-42a4-a2cc-836ec5cffe25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "       CUSTOM LAYER: SPECTRAL NORMALIZATION\n",
            "============================================================\n",
            "\n",
            "SpectralNorm(32, 64)\n",
            "Input shape:  torch.Size([4, 32])\n",
            "Output shape: torch.Size([4, 64])\n",
            "\n",
            " Use case: Stabilize GAN discriminator training\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    CUSTOM LAYER: SPECTRAL NORMALIZATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"       CUSTOM LAYER: SPECTRAL NORMALIZATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class SpectralNorm(nn.Module):\n",
        "    \"\"\"\n",
        "    Spectral Normalization wrapper for linear layers.\n",
        "\n",
        "    Constrains the spectral norm (largest singular value) of the weight matrix.\n",
        "    Used in GANs to stabilize training.\n",
        "\n",
        "    W_normalized = W / sigma(W)\n",
        "    where sigma(W) is the largest singular value.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, power_iterations=1):\n",
        "        super().__init__()\n",
        "        self.power_iterations = power_iterations\n",
        "\n",
        "        # Weight matrix\n",
        "        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n",
        "        self.bias = nn.Parameter(torch.zeros(out_features))\n",
        "\n",
        "        # Register buffers for u and v vectors (not parameters, but saved in state_dict)\n",
        "        self.register_buffer('u', torch.randn(out_features))\n",
        "        self.register_buffer('v', torch.randn(in_features))\n",
        "\n",
        "    def _power_iteration(self):\n",
        "        \"\"\"Estimate largest singular value using power iteration.\"\"\"\n",
        "        u = self.u\n",
        "        v = self.v\n",
        "\n",
        "        for _ in range(self.power_iterations):\n",
        "            # v = W^T u / ||W^T u||\n",
        "            v = F.normalize(self.weight.t() @ u, dim=0)\n",
        "            # u = W v / ||W v||\n",
        "            u = F.normalize(self.weight @ v, dim=0)\n",
        "\n",
        "        # Update buffers (detached)\n",
        "        self.u.copy_(u.detach())\n",
        "        self.v.copy_(v.detach())\n",
        "\n",
        "        # Spectral norm: sigma = u^T W v\n",
        "        sigma = torch.dot(u, self.weight @ v)\n",
        "        return sigma\n",
        "\n",
        "    def forward(self, x):\n",
        "        sigma = self._power_iteration()\n",
        "        weight_normalized = self.weight / sigma\n",
        "        return F.linear(x, weight_normalized, self.bias)\n",
        "\n",
        "# Test\n",
        "spectral_layer = SpectralNorm(32, 64)\n",
        "x = torch.randn(4, 32)\n",
        "y = spectral_layer(x)\n",
        "\n",
        "print(f\"\\nSpectralNorm(32, 64)\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"\\n Use case: Stabilize GAN discriminator training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQJeSE01130p"
      },
      "source": [
        "---\n",
        "\n",
        "# Part V: Advanced Architectures\n",
        "\n",
        "## Building Modern Deep Learning Components\n",
        "\n",
        "Now let's build advanced architectural components used in state-of-the-art models:\n",
        "\n",
        "- **Residual Blocks** (ResNet)\n",
        "- **Squeeze-and-Excitation** (SENet)\n",
        "- **Transformer Encoder Block**\n",
        "- **Custom Normalization Layers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQuurStD130p",
        "outputId": "a464cc83-ecff-4eb0-a0e9-a838e93709ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "           RESIDUAL BLOCK (ResNet)\n",
            "============================================================\n",
            "\n",
            "ResidualBlock(64, 64)\n",
            "Input shape:  torch.Size([2, 64, 32, 32])\n",
            "Output shape: torch.Size([2, 64, 32, 32])\n",
            "\n",
            "ResidualBlock(64, 128, stride=2)\n",
            "Output shape: torch.Size([2, 128, 16, 16])  (spatial dims halved, channels doubled)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    RESIDUAL BLOCK (ResNet Style)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"           RESIDUAL BLOCK (ResNet)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Residual Block: y = F(x) + x\n",
        "\n",
        "    The key insight: learning residual F(x) = y - x is easier than learning y directly.\n",
        "    This enables training very deep networks (100+ layers).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Main path\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        # Skip connection (identity or projection)\n",
        "        self.skip = nn.Identity()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.skip = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1,\n",
        "                          stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Main path\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "\n",
        "        # Skip connection + activation\n",
        "        return F.relu(out + self.skip(x))\n",
        "\n",
        "# Test\n",
        "res_block = ResidualBlock(64, 64, stride=1)\n",
        "x = torch.randn(2, 64, 32, 32)\n",
        "y = res_block(x)\n",
        "\n",
        "print(f\"\\nResidualBlock(64, 64)\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "\n",
        "# With downsampling\n",
        "res_block_down = ResidualBlock(64, 128, stride=2)\n",
        "y_down = res_block_down(x)\n",
        "print(f\"\\nResidualBlock(64, 128, stride=2)\")\n",
        "print(f\"Output shape: {y_down.shape}  (spatial dims halved, channels doubled)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IB1DJ8LX130p",
        "outputId": "5b666ef5-d33d-4f0e-c703-eb121b61d0e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "         SQUEEZE-AND-EXCITATION BLOCK\n",
            "============================================================\n",
            "\n",
            "SqueezeExcitationBlock(channels=64, reduction_ratio=16)\n",
            "Input shape:  torch.Size([2, 64, 28, 28])\n",
            "Output shape: torch.Size([2, 64, 28, 28])\n",
            "\n",
            " SE learns which channels are important for the task\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    SQUEEZE-AND-EXCITATION BLOCK\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"         SQUEEZE-AND-EXCITATION BLOCK\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class SqueezeExcitationBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Squeeze-and-Excitation (SE) Block.\n",
        "\n",
        "    Learns channel-wise attention weights:\n",
        "    1. Squeeze: Global average pooling (B,C,H,W) -> (B,C,1,1)\n",
        "    2. Excitation: FC -> ReLU -> FC -> Sigmoid\n",
        "    3. Scale: Multiply input by attention weights\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channels, reduction_ratio=16):\n",
        "        super().__init__()\n",
        "        reduced_channels = max(channels // reduction_ratio, 1)\n",
        "\n",
        "        self.squeeze = nn.AdaptiveAvgPool2d(1)\n",
        "        self.excitation = nn.Sequential(\n",
        "            nn.Linear(channels, reduced_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(reduced_channels, channels),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, channels, _, _ = x.shape\n",
        "\n",
        "        # Squeeze: Global Average Pooling\n",
        "        squeezed = self.squeeze(x).view(batch_size, channels)\n",
        "\n",
        "        # Excitation: Learn channel importance\n",
        "        attention = self.excitation(squeezed).view(batch_size, channels, 1, 1)\n",
        "\n",
        "        # Scale: Apply attention\n",
        "        return x * attention\n",
        "\n",
        "# Test\n",
        "se_block = SqueezeExcitationBlock(channels=64, reduction_ratio=16)\n",
        "x = torch.randn(2, 64, 28, 28)\n",
        "y = se_block(x)\n",
        "\n",
        "print(f\"\\nSqueezeExcitationBlock(channels=64, reduction_ratio=16)\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"\\n SE learns which channels are important for the task\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrWgtJfV130p",
        "outputId": "fabee4bb-9963-4eda-930d-a1e1e39d3908"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "          TRANSFORMER ENCODER BLOCK\n",
            "============================================================\n",
            "\n",
            "TransformerEncoderBlock(embed_dim=64, num_heads=4, ff_dim=256)\n",
            "Input shape:  torch.Size([2, 20, 64])\n",
            "Output shape: torch.Size([2, 20, 64])\n",
            "Parameters:   49,984\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    TRANSFORMER ENCODER BLOCK\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"          TRANSFORMER ENCODER BLOCK\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer Encoder Block.\n",
        "\n",
        "    Architecture (Pre-Norm):\n",
        "        x -> LayerNorm -> MultiHeadAttention -> + (residual) ->\n",
        "             LayerNorm -> FeedForward -> + (residual) -> output\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Multi-head attention\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            embed_dim=embed_dim,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Feed-forward network\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(embed_dim, ff_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(ff_dim, embed_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        # Layer normalization\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Pre-norm architecture (more stable)\n",
        "        # Attention block\n",
        "        normed = self.norm1(x)\n",
        "        attn_output, _ = self.attention(normed, normed, normed, attn_mask=mask)\n",
        "        x = x + self.dropout(attn_output)  # Residual connection\n",
        "\n",
        "        # Feed-forward block\n",
        "        normed = self.norm2(x)\n",
        "        ff_output = self.ffn(normed)\n",
        "        x = x + ff_output  # Residual connection\n",
        "\n",
        "        return x\n",
        "\n",
        "# Test\n",
        "transformer_block = TransformerEncoderBlock(\n",
        "    embed_dim=64,\n",
        "    num_heads=4,\n",
        "    ff_dim=256,\n",
        "    dropout=0.1\n",
        ")\n",
        "\n",
        "x = torch.randn(2, 20, 64)  # (batch, seq_len, embed_dim)\n",
        "y = transformer_block(x)\n",
        "\n",
        "print(f\"\\nTransformerEncoderBlock(embed_dim=64, num_heads=4, ff_dim=256)\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"Parameters:   {sum(p.numel() for p in transformer_block.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmeC-7iT130p"
      },
      "source": [
        "---\n",
        "\n",
        "# Part VI: Custom Training Loops\n",
        "\n",
        "## Full Control Over Training\n",
        "\n",
        "While high-level training abstractions are convenient, custom training loops give you:\n",
        "\n",
        "- **Complete control** over the training process\n",
        "- **Custom metrics** and logging\n",
        "- **Complex training schemes** (GANs, reinforcement learning)\n",
        "- **Gradient manipulation** (clipping, accumulation)\n",
        "- **Multi-GPU/TPU** strategies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kteN0deX130p",
        "outputId": "796756a8-72db-43db-b71a-dc7dd44bafbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "          BASIC CUSTOM TRAINING LOOP\n",
            "============================================================\n",
            "\n",
            "Custom training loop template created!\n",
            "Key components:\n",
            "  1. model.train() - enables dropout, batch norm training mode\n",
            "  2. optimizer.zero_grad() - clears old gradients\n",
            "  3. loss.backward() - computes gradients\n",
            "  4. optimizer.step() - updates weights\n",
            "  5. model.eval() + torch.no_grad() - for validation\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    BASIC CUSTOM TRAINING LOOP\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"          BASIC CUSTOM TRAINING LOOP\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def custom_training_loop(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    epochs,\n",
        "    learning_rate=0.001,\n",
        "    device='cpu'\n",
        "):\n",
        "    \"\"\"\n",
        "    Complete custom training loop.\n",
        "    \"\"\"\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # ----- TRAINING PHASE -----\n",
        "        model.train()\n",
        "        train_loss, train_correct, train_total = 0, 0, 0\n",
        "\n",
        "        for x_batch, y_batch in train_loader:\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(x_batch)\n",
        "            loss = loss_fn(outputs, y_batch)\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Track metrics\n",
        "            train_loss += loss.item() * x_batch.size(0)\n",
        "            train_correct += (outputs.argmax(dim=1) == y_batch).sum().item()\n",
        "            train_total += x_batch.size(0)\n",
        "\n",
        "        # ----- VALIDATION PHASE -----\n",
        "        model.eval()\n",
        "        val_loss, val_correct, val_total = 0, 0, 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for x_batch, y_batch in val_loader:\n",
        "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "                outputs = model(x_batch)\n",
        "                loss = loss_fn(outputs, y_batch)\n",
        "\n",
        "                val_loss += loss.item() * x_batch.size(0)\n",
        "                val_correct += (outputs.argmax(dim=1) == y_batch).sum().item()\n",
        "                val_total += x_batch.size(0)\n",
        "\n",
        "        # Record history\n",
        "        history['train_loss'].append(train_loss / train_total)\n",
        "        history['train_acc'].append(train_correct / train_total)\n",
        "        history['val_loss'].append(val_loss / val_total)\n",
        "        history['val_acc'].append(val_correct / val_total)\n",
        "\n",
        "        # Print progress\n",
        "        if (epoch + 1) % max(1, epochs // 10) == 0:\n",
        "            print(f\"Epoch {epoch+1:4d}/{epochs} | \"\n",
        "                  f\"Train Loss: {history['train_loss'][-1]:.4f} | \"\n",
        "                  f\"Train Acc: {history['train_acc'][-1]:.4f} | \"\n",
        "                  f\"Val Loss: {history['val_loss'][-1]:.4f} | \"\n",
        "                  f\"Val Acc: {history['val_acc'][-1]:.4f}\")\n",
        "\n",
        "    return history\n",
        "\n",
        "print(\"\\nCustom training loop template created!\")\n",
        "print(\"Key components:\")\n",
        "print(\"  1. model.train() - enables dropout, batch norm training mode\")\n",
        "print(\"  2. optimizer.zero_grad() - clears old gradients\")\n",
        "print(\"  3. loss.backward() - computes gradients\")\n",
        "print(\"  4. optimizer.step() - updates weights\")\n",
        "print(\"  5. model.eval() + torch.no_grad() - for validation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTGa8i9A130p",
        "outputId": "02684f98-0877-485a-b4a5-c7ed97da8dd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "        TRAINING WITH GRADIENT CLIPPING\n",
            "============================================================\n",
            "Gradient Clipping Options in PyTorch:\n",
            "  - torch.nn.utils.clip_grad_value_(params, clip): Clip element-wise\n",
            "  - torch.nn.utils.clip_grad_norm_(params, max_norm): Clip by global norm\n",
            "\n",
            "Global norm is most common - maintains gradient direction!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    TRAINING WITH GRADIENT CLIPPING\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"        TRAINING WITH GRADIENT CLIPPING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def train_step_with_clipping(model, x, y, optimizer, loss_fn, clip_norm=1.0):\n",
        "    \"\"\"\n",
        "    Single training step with gradient clipping.\n",
        "\n",
        "    Gradient clipping prevents exploding gradients in deep networks.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(x)\n",
        "    loss = loss_fn(outputs, y)\n",
        "\n",
        "    # Backward pass\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # Clip gradients by global norm\n",
        "    total_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), clip_norm)\n",
        "\n",
        "    # Apply clipped gradients\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item(), total_norm.item()\n",
        "\n",
        "print(\"Gradient Clipping Options in PyTorch:\")\n",
        "print(\"  - torch.nn.utils.clip_grad_value_(params, clip): Clip element-wise\")\n",
        "print(\"  - torch.nn.utils.clip_grad_norm_(params, max_norm): Clip by global norm\")\n",
        "print(\"\\nGlobal norm is most common - maintains gradient direction!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWvo8bzn130q",
        "outputId": "04c85e41-2fd9-41da-d187-2fa952bcc39d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "        LEARNING RATE SCHEDULING\n",
            "============================================================\n",
            "\n",
            "Available LR Schedulers:\n",
            "  - StepLR\n",
            "  - ExponentialLR\n",
            "  - CosineAnnealingLR\n",
            "  - ReduceLROnPlateau\n",
            "  - OneCycleLR\n",
            "\n",
            "Usage in training loop:\n",
            "  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
            "  for epoch in range(epochs):\n",
            "      train_one_epoch(...)\n",
            "      scheduler.step()  # Update learning rate\n",
            "\n",
            "Linear Warmup Pattern:\n",
            "  warmup_steps = 1000\n",
            "  for step in range(warmup_steps):\n",
            "      lr = base_lr * (step / warmup_steps)\n",
            "      for param_group in optimizer.param_groups:\n",
            "          param_group['lr'] = lr\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#            LEARNING RATE SCHEDULING AND WARMUP\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"        LEARNING RATE SCHEDULING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# PyTorch provides many schedulers!\n",
        "model = nn.Linear(10, 2)  # Dummy model\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Common schedulers\n",
        "schedulers = {\n",
        "    'StepLR': torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1),\n",
        "    'ExponentialLR': torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95),\n",
        "    'CosineAnnealingLR': torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50),\n",
        "    'ReduceLROnPlateau': torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5),\n",
        "    'OneCycleLR': torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, total_steps=100),\n",
        "}\n",
        "\n",
        "print(\"\\nAvailable LR Schedulers:\")\n",
        "for name in schedulers:\n",
        "    print(f\"  - {name}\")\n",
        "\n",
        "print(\"\\nUsage in training loop:\")\n",
        "print(\"  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\")\n",
        "print(\"  for epoch in range(epochs):\")\n",
        "print(\"      train_one_epoch(...)\")\n",
        "print(\"      scheduler.step()  # Update learning rate\")\n",
        "\n",
        "# Warmup example\n",
        "print(\"\\nLinear Warmup Pattern:\")\n",
        "print(\"  warmup_steps = 1000\")\n",
        "print(\"  for step in range(warmup_steps):\")\n",
        "print(\"      lr = base_lr * (step / warmup_steps)\")\n",
        "print(\"      for param_group in optimizer.param_groups:\")\n",
        "print(\"          param_group['lr'] = lr\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5E8_Cmh130q"
      },
      "source": [
        "---\n",
        "\n",
        "# Part VII: Practical Demos\n",
        "\n",
        "## Putting It All Together\n",
        "\n",
        "Let's combine everything we've learned in a real example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6i9U7z-130q",
        "outputId": "a06039d3-51fb-4543-d6f1-e9a4bcd696a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "     DEMO: CUSTOM RESNET FOR DIGIT CLASSIFICATION\n",
            "============================================================\n",
            "Training samples: 1437\n",
            "Test samples:     360\n",
            "Image shape:      torch.Size([1, 8, 8])\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#              DEMO: CUSTOM RESNET FOR DIGIT CLASSIFICATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"     DEMO: CUSTOM RESNET FOR DIGIT CLASSIFICATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load digits dataset\n",
        "digits = load_digits()\n",
        "X, y = digits.data, digits.target\n",
        "\n",
        "# Preprocess\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Reshape for CNN: (samples, 1, 8, 8)\n",
        "X = X.reshape(-1, 1, 8, 8).astype(np.float32)\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to tensors\n",
        "X_train = torch.tensor(X_train)\n",
        "X_test = torch.tensor(X_test)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "print(f\"Training samples: {X_train.shape[0]}\")\n",
        "print(f\"Test samples:     {X_test.shape[0]}\")\n",
        "print(f\"Image shape:      {X_train.shape[1:]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uqoufajq130q",
        "outputId": "fd7dfdcf-4325-40ab-da63-17c0f20f43c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MiniResNet Architecture:\n",
            "MiniResNet(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (res_block1): ResidualBlock(\n",
            "    (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (skip): Identity()\n",
            "  )\n",
            "  (res_block2): ResidualBlock(\n",
            "    (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (skip): Sequential(\n",
            "      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (se_block): SqueezeExcitationBlock(\n",
            "    (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
            "    (excitation): Sequential(\n",
            "      (0): Linear(in_features=64, out_features=8, bias=True)\n",
            "      (1): ReLU(inplace=True)\n",
            "      (2): Linear(in_features=8, out_features=64, bias=True)\n",
            "      (3): Sigmoid()\n",
            "    )\n",
            "  )\n",
            "  (global_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
            ")\n",
            "\n",
            "Total parameters: 78,418\n"
          ]
        }
      ],
      "source": [
        "# Build custom ResNet model\n",
        "class MiniResNet(nn.Module):\n",
        "    \"\"\"Mini ResNet with custom residual blocks.\"\"\"\n",
        "\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "\n",
        "        # Initial convolution\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "\n",
        "        # Residual blocks\n",
        "        self.res_block1 = ResidualBlock(32, 32)\n",
        "        self.res_block2 = ResidualBlock(32, 64, stride=2)\n",
        "\n",
        "        # SE block for channel attention\n",
        "        self.se_block = SqueezeExcitationBlock(64, reduction_ratio=8)\n",
        "\n",
        "        # Classification head\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc = nn.Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initial conv\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "\n",
        "        # Residual blocks\n",
        "        x = self.res_block1(x)\n",
        "        x = self.res_block2(x)\n",
        "\n",
        "        # SE attention\n",
        "        x = self.se_block(x)\n",
        "\n",
        "        # Classification head\n",
        "        x = self.global_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Create model\n",
        "torch.manual_seed(42)\n",
        "resnet_model = MiniResNet(num_classes=10).to(device)\n",
        "\n",
        "print(\"\\nMiniResNet Architecture:\")\n",
        "print(resnet_model)\n",
        "print(f\"\\nTotal parameters: {sum(p.numel() for p in resnet_model.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKewny5v130q",
        "outputId": "a9535073-e2db-4e8a-ad00-bbf721f8a757"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training MiniResNet...\n",
            "\n",
            "Epoch  5/30 | Train Loss: 0.0738 | Train Acc: 0.9958 | Val Acc: 0.9639\n",
            "Epoch 10/30 | Train Loss: 0.0238 | Train Acc: 0.9986 | Val Acc: 0.9889\n",
            "Epoch 15/30 | Train Loss: 0.0128 | Train Acc: 0.9986 | Val Acc: 0.9944\n",
            "Epoch 20/30 | Train Loss: 0.0053 | Train Acc: 1.0000 | Val Acc: 0.9944\n",
            "Epoch 25/30 | Train Loss: 0.0046 | Train Acc: 1.0000 | Val Acc: 0.9944\n",
            "Epoch 30/30 | Train Loss: 0.0047 | Train Acc: 1.0000 | Val Acc: 0.9917\n",
            "\n",
            "Final Test Accuracy: 99.17%\n"
          ]
        }
      ],
      "source": [
        "# Create data loaders\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Training setup\n",
        "optimizer = torch.optim.Adam(resnet_model.parameters(), lr=0.001)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "print(\"\\nTraining MiniResNet...\\n\")\n",
        "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "\n",
        "for epoch in range(30):\n",
        "    # Training\n",
        "    resnet_model.train()\n",
        "    train_loss, train_correct, train_total = 0, 0, 0\n",
        "\n",
        "    for x_batch, y_batch in train_loader:\n",
        "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = resnet_model(x_batch)\n",
        "        loss = loss_fn(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * x_batch.size(0)\n",
        "        train_correct += (outputs.argmax(dim=1) == y_batch).sum().item()\n",
        "        train_total += x_batch.size(0)\n",
        "\n",
        "    # Validation\n",
        "    resnet_model.eval()\n",
        "    val_loss, val_correct, val_total = 0, 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x_batch, y_batch in test_loader:\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "            outputs = resnet_model(x_batch)\n",
        "            loss = loss_fn(outputs, y_batch)\n",
        "\n",
        "            val_loss += loss.item() * x_batch.size(0)\n",
        "            val_correct += (outputs.argmax(dim=1) == y_batch).sum().item()\n",
        "            val_total += x_batch.size(0)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    # Record\n",
        "    history['train_loss'].append(train_loss / train_total)\n",
        "    history['train_acc'].append(train_correct / train_total)\n",
        "    history['val_loss'].append(val_loss / val_total)\n",
        "    history['val_acc'].append(val_correct / val_total)\n",
        "\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        print(f\"Epoch {epoch+1:2d}/30 | \"\n",
        "              f\"Train Loss: {history['train_loss'][-1]:.4f} | \"\n",
        "              f\"Train Acc: {history['train_acc'][-1]:.4f} | \"\n",
        "              f\"Val Acc: {history['val_acc'][-1]:.4f}\")\n",
        "\n",
        "print(f\"\\nFinal Test Accuracy: {history['val_acc'][-1]*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8iNpgt2m130q",
        "outputId": "06b77d2f-3014-46d1-d9e4-99a66848603a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1400x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x600 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    VISUALIZE TRAINING RESULTS\n",
        "# ============================================================================\n",
        "\n",
        "# Plot training history\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "ax1.plot(history['train_loss'], 'b-', label='Train Loss')\n",
        "ax1.plot(history['val_loss'], 'r-', label='Val Loss')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.set_title('MiniResNet Training Loss', fontweight='bold')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "ax2.plot(history['train_acc'], 'b-', label='Train Acc')\n",
        "ax2.plot(history['val_acc'], 'r-', label='Val Acc')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.set_title('MiniResNet Training Accuracy', fontweight='bold')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Visualize predictions\n",
        "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
        "fig.suptitle('MiniResNet Predictions', fontsize=14, fontweight='bold')\n",
        "\n",
        "resnet_model.eval()\n",
        "with torch.no_grad():\n",
        "    X_sample = X_test[:10].to(device)\n",
        "    predictions = resnet_model(X_sample)\n",
        "    probs = F.softmax(predictions, dim=1)\n",
        "    pred_classes = predictions.argmax(dim=1).cpu()\n",
        "\n",
        "for i, ax in enumerate(axes.flatten()):\n",
        "    img = X_test[i, 0].numpy()\n",
        "    true_label = y_test[i].item()\n",
        "    pred_label = pred_classes[i].item()\n",
        "    confidence = probs[i, pred_label].item() * 100\n",
        "\n",
        "    ax.imshow(img, cmap='gray')\n",
        "    color = 'green' if true_label == pred_label else 'red'\n",
        "    ax.set_title(f'True: {true_label}, Pred: {pred_label}\\n({confidence:.1f}%)',\n",
        "                 color=color, fontsize=10)\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJz3WzGk130q",
        "outputId": "61475761-183b-492e-9d67-1c9ae3767950"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "                 ADVANCED PYTORCH CHEAT SHEET\n",
            "======================================================================\n",
            "\n",
            "AUTOGRAD PATTERNS\n",
            "-----------------\n",
            "# Basic gradient\n",
            "y = model(x)\n",
            "y.backward()\n",
            "grads = [p.grad for p in model.parameters()]\n",
            "\n",
            "# Higher-order derivatives\n",
            "dy_dx = torch.autograd.grad(y, x, create_graph=True)[0]\n",
            "d2y_dx2 = torch.autograd.grad(dy_dx, x)[0]\n",
            "\n",
            "# Jacobian and Hessian\n",
            "jacobian = torch.autograd.functional.jacobian(f, x)\n",
            "hessian = torch.autograd.functional.hessian(f, x)\n",
            "\n",
            "# Custom autograd function\n",
            "class CustomOp(torch.autograd.Function):\n",
            "    @staticmethod\n",
            "    def forward(ctx, x):\n",
            "        ctx.save_for_backward(x)\n",
            "        return forward_result\n",
            "    @staticmethod\n",
            "    def backward(ctx, grad_output):\n",
            "        x, = ctx.saved_tensors\n",
            "        return custom_gradient\n",
            "\n",
            "CUSTOM nn.Module LAYERS\n",
            "-----------------------\n",
            "class CustomLayer(nn.Module):\n",
            "    def __init__(self, in_features, out_features):\n",
            "        super().__init__()\n",
            "        self.weight = nn.Parameter(torch.randn(in_features, out_features))\n",
            "        self.bias = nn.Parameter(torch.zeros(out_features))\n",
            "\n",
            "    def forward(self, x):\n",
            "        return x @ self.weight + self.bias\n",
            "\n",
            "CUSTOM TRAINING\n",
            "---------------\n",
            "# Training loop\n",
            "model.train()\n",
            "for x, y in train_loader:\n",
            "    optimizer.zero_grad()\n",
            "    loss = loss_fn(model(x), y)\n",
            "    loss.backward()\n",
            "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
            "    optimizer.step()\n",
            "scheduler.step()\n",
            "\n",
            "# Validation loop\n",
            "model.eval()\n",
            "with torch.no_grad():\n",
            "    for x, y in val_loader:\n",
            "        outputs = model(x)\n",
            "\n",
            "GRADIENT MANIPULATION\n",
            "---------------------\n",
            "# Clip by global norm\n",
            "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
            "\n",
            "# Gradient accumulation\n",
            "for i, (x, y) in enumerate(loader):\n",
            "    loss = loss_fn(model(x), y) / accumulation_steps\n",
            "    loss.backward()\n",
            "    if (i + 1) % accumulation_steps == 0:\n",
            "        optimizer.step()\n",
            "        optimizer.zero_grad()\n",
            "\n",
            "# Gradient hooks\n",
            "handle = tensor.register_hook(lambda grad: grad.clamp(-1, 1))\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                         ADVANCED CHEAT SHEET\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"                 ADVANCED PYTORCH CHEAT SHEET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "cheat_sheet = \"\"\"\n",
        "AUTOGRAD PATTERNS\n",
        "-----------------\n",
        "# Basic gradient\n",
        "y = model(x)\n",
        "y.backward()\n",
        "grads = [p.grad for p in model.parameters()]\n",
        "\n",
        "# Higher-order derivatives\n",
        "dy_dx = torch.autograd.grad(y, x, create_graph=True)[0]\n",
        "d2y_dx2 = torch.autograd.grad(dy_dx, x)[0]\n",
        "\n",
        "# Jacobian and Hessian\n",
        "jacobian = torch.autograd.functional.jacobian(f, x)\n",
        "hessian = torch.autograd.functional.hessian(f, x)\n",
        "\n",
        "# Custom autograd function\n",
        "class CustomOp(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x):\n",
        "        ctx.save_for_backward(x)\n",
        "        return forward_result\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        x, = ctx.saved_tensors\n",
        "        return custom_gradient\n",
        "\n",
        "CUSTOM nn.Module LAYERS\n",
        "-----------------------\n",
        "class CustomLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.randn(in_features, out_features))\n",
        "        self.bias = nn.Parameter(torch.zeros(out_features))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x @ self.weight + self.bias\n",
        "\n",
        "CUSTOM TRAINING\n",
        "---------------\n",
        "# Training loop\n",
        "model.train()\n",
        "for x, y in train_loader:\n",
        "    optimizer.zero_grad()\n",
        "    loss = loss_fn(model(x), y)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "scheduler.step()\n",
        "\n",
        "# Validation loop\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for x, y in val_loader:\n",
        "        outputs = model(x)\n",
        "\n",
        "GRADIENT MANIPULATION\n",
        "---------------------\n",
        "# Clip by global norm\n",
        "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "# Gradient accumulation\n",
        "for i, (x, y) in enumerate(loader):\n",
        "    loss = loss_fn(model(x), y) / accumulation_steps\n",
        "    loss.backward()\n",
        "    if (i + 1) % accumulation_steps == 0:\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "# Gradient hooks\n",
        "handle = tensor.register_hook(lambda grad: grad.clamp(-1, 1))\n",
        "\"\"\"\n",
        "print(cheat_sheet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jguqJC8p130q"
      },
      "source": [
        "---\n",
        "\n",
        "# Conclusion\n",
        "\n",
        "## Your Advanced PyTorch Journey\n",
        "\n",
        "Congratulations! You've mastered advanced PyTorch techniques.\n",
        "\n",
        "### What You Learned\n",
        "\n",
        "| Part | Topic | Key Takeaway |\n",
        "|------|-------|-------------|\n",
        "| I | Advanced Autograd | Nested autograd, Jacobians, custom functions |\n",
        "| II | Building Ops | Conv, pooling, normalization from scratch |\n",
        "| III | Primitive Layers | Dense, Conv2D with only basic tensors |\n",
        "| IV | Custom nn.Module | Proper subclassing with nn.Parameter |\n",
        "| V | Advanced Architectures | ResNet, SE-Net, Transformer blocks |\n",
        "| VI | Custom Training | Full control with manual training loops |\n",
        "| VII | Practical Demos | Real-world model combining everything |\n",
        "\n",
        "### When to Use What\n",
        "\n",
        "| Approach | Use When |\n",
        "|----------|----------|\n",
        "| Standard `nn.Module` | Most deep learning tasks |\n",
        "| Custom `autograd.Function` | Non-differentiable ops, custom gradients |\n",
        "| Custom training loop | GANs, RL, complex multi-model training |\n",
        "| Gradient hooks | Debugging, visualization, per-layer manipulation |\n",
        "| Primitive layers | Learning, debugging, maximum control |\n",
        "\n",
        "### The Complete Learning Path\n",
        "\n",
        "1. **NumPy from Scratch** - Understand the math deeply\n",
        "2. **PyTorch Part 1** - Fundamentals and high-level API\n",
        "3. **PyTorch Part 2** - Advanced custom components (This notebook!)\n",
        "4. **TensorFlow/Keras** - Alternative framework comparison\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- **Vision Transformers (ViT)** - Transformers for images\n",
        "- **Diffusion Models** - State-of-the-art generative AI\n",
        "- **Distributed Training** - Multi-GPU with DDP\n",
        "- **Model Optimization** - TorchScript, quantization, pruning\n",
        "- **PyTorch Lightning** - Even cleaner training code\n",
        "\n",
        "---\n",
        "\n",
        "*\"The more you understand the primitives, the better you can innovate.\"*\n",
        "\n",
        "**Happy Deep Learning!**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}